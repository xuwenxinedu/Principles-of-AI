{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tianchi_CNN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMJ5ExdAYPN0GpS17BokL1k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xuwenxinedu/Principles-of-AI/blob/main/Tianchi_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yg0HxgKQGY4H",
        "outputId": "9b6b84ce-604f-4fc7-efca-6c8d410d8c4a"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "%cd /gdrive/My Drive"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "/gdrive/My Drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hal5QcAG-qD",
        "outputId": "7402050f-2626-44e7-dd39-318c37475e26"
      },
      "source": [
        "%cd /gdrive/My Drive\n",
        "%cd COLAB"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/gdrive/My Drive\n",
            "/gdrive/My Drive/COLAB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LoIbYn4pHJ0y",
        "outputId": "22a646c5-2786-4a97-8fab-3bc84094f8d3"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hy_round1_train_20200102  npy\t    Tianchi_CNN.ipynb\n",
            "new.ipynb\t\t  RF.ipynb  train.feather\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_zaTUzNHM7T"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn.metrics as mtr\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import Dense\n",
        "from keras.models import Sequential\n",
        "from keras.callbacks import Callback, EarlyStopping\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Concatenate, Reshape, Dropout, merge, Add\n",
        "from keras.layers.embeddings import Embedding\n",
        "from sklearn.model_selection import KFold, GroupKFold, StratifiedKFold\n",
        "import warnings\n",
        "import random as rn\n",
        "import tensorflow as tf\n",
        "from keras.models import load_model\n",
        "import os\n",
        "from sklearn.metrics import f1_score\n",
        "from collections import Counter\n",
        "import keras\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6m27WNngHRMj"
      },
      "source": [
        "train = 'train.feather'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7bH4l-NHdyZ"
      },
      "source": [
        "train = pd.read_feather(train)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "id": "hQsygVsBHxtS",
        "outputId": "322c7acc-2629-4718-8757-aad4de7fc7f0"
      },
      "source": [
        "train[train.isna().T.any()]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>渔船ID</th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "      <th>速度</th>\n",
              "      <th>方向</th>\n",
              "      <th>time</th>\n",
              "      <th>type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [渔船ID, x, y, 速度, 方向, time, type]\n",
              "Index: []"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "F73QcJ6AIMTq",
        "outputId": "cd4038c3-2b47-46d8-ca99-b6a72744eab5"
      },
      "source": [
        "\n",
        "train.columns = ['id', 'x', 'y', 'speed', 'ori', 'time', 'type']\n",
        "train.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "      <th>speed</th>\n",
              "      <th>ori</th>\n",
              "      <th>time</th>\n",
              "      <th>type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>6.152038e+06</td>\n",
              "      <td>5.124873e+06</td>\n",
              "      <td>2.59</td>\n",
              "      <td>102</td>\n",
              "      <td>1110 11:58:19</td>\n",
              "      <td>拖网</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>6.151230e+06</td>\n",
              "      <td>5.125218e+06</td>\n",
              "      <td>2.70</td>\n",
              "      <td>113</td>\n",
              "      <td>1110 11:48:19</td>\n",
              "      <td>拖网</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>6.150421e+06</td>\n",
              "      <td>5.125563e+06</td>\n",
              "      <td>2.70</td>\n",
              "      <td>116</td>\n",
              "      <td>1110 11:38:19</td>\n",
              "      <td>拖网</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>6.149612e+06</td>\n",
              "      <td>5.125907e+06</td>\n",
              "      <td>3.29</td>\n",
              "      <td>95</td>\n",
              "      <td>1110 11:28:19</td>\n",
              "      <td>拖网</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>6.148803e+06</td>\n",
              "      <td>5.126252e+06</td>\n",
              "      <td>3.18</td>\n",
              "      <td>108</td>\n",
              "      <td>1110 11:18:19</td>\n",
              "      <td>拖网</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id             x             y  speed  ori           time type\n",
              "0   0  6.152038e+06  5.124873e+06   2.59  102  1110 11:58:19   拖网\n",
              "1   0  6.151230e+06  5.125218e+06   2.70  113  1110 11:48:19   拖网\n",
              "2   0  6.150421e+06  5.125563e+06   2.70  116  1110 11:38:19   拖网\n",
              "3   0  6.149612e+06  5.125907e+06   3.29   95  1110 11:28:19   拖网\n",
              "4   0  6.148803e+06  5.126252e+06   3.18  108  1110 11:18:19   拖网"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4y-Uzxy8I-lS",
        "outputId": "dfdaa509-5fbe-4db9-98d7-63e01174c005"
      },
      "source": [
        "train.type.unique()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['拖网', '围网', '刺网'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "DyInNx9oJHQC",
        "outputId": "84f29b0d-269a-4c46-f44d-4b5e59ab1fde"
      },
      "source": [
        "type_dict = {'围网':0, '拖网':1, '刺网':2}\n",
        "type_dict_inverse = {0:'围网', 1:'拖网', 2:'刺网'}\n",
        "\n",
        "train.type = train.type.map(type_dict)\n",
        "train"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "      <th>speed</th>\n",
              "      <th>ori</th>\n",
              "      <th>time</th>\n",
              "      <th>type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>6.152038e+06</td>\n",
              "      <td>5.124873e+06</td>\n",
              "      <td>2.59</td>\n",
              "      <td>102</td>\n",
              "      <td>1110 11:58:19</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>6.151230e+06</td>\n",
              "      <td>5.125218e+06</td>\n",
              "      <td>2.70</td>\n",
              "      <td>113</td>\n",
              "      <td>1110 11:48:19</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>6.150421e+06</td>\n",
              "      <td>5.125563e+06</td>\n",
              "      <td>2.70</td>\n",
              "      <td>116</td>\n",
              "      <td>1110 11:38:19</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>6.149612e+06</td>\n",
              "      <td>5.125907e+06</td>\n",
              "      <td>3.29</td>\n",
              "      <td>95</td>\n",
              "      <td>1110 11:28:19</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>6.148803e+06</td>\n",
              "      <td>5.126252e+06</td>\n",
              "      <td>3.18</td>\n",
              "      <td>108</td>\n",
              "      <td>1110 11:18:19</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2699633</th>\n",
              "      <td>999</td>\n",
              "      <td>6.138413e+06</td>\n",
              "      <td>5.162715e+06</td>\n",
              "      <td>0.32</td>\n",
              "      <td>40</td>\n",
              "      <td>1031 13:09:00</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2699634</th>\n",
              "      <td>999</td>\n",
              "      <td>6.138412e+06</td>\n",
              "      <td>5.162606e+06</td>\n",
              "      <td>0.22</td>\n",
              "      <td>275</td>\n",
              "      <td>1031 12:48:58</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2699635</th>\n",
              "      <td>999</td>\n",
              "      <td>6.138413e+06</td>\n",
              "      <td>5.162715e+06</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0</td>\n",
              "      <td>1031 12:28:01</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2699636</th>\n",
              "      <td>999</td>\n",
              "      <td>6.138413e+06</td>\n",
              "      <td>5.162715e+06</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0</td>\n",
              "      <td>1031 12:18:00</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2699637</th>\n",
              "      <td>999</td>\n",
              "      <td>6.138413e+06</td>\n",
              "      <td>5.162715e+06</td>\n",
              "      <td>0.11</td>\n",
              "      <td>294</td>\n",
              "      <td>1031 12:07:59</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2699638 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          id             x             y  speed  ori           time  type\n",
              "0          0  6.152038e+06  5.124873e+06   2.59  102  1110 11:58:19     1\n",
              "1          0  6.151230e+06  5.125218e+06   2.70  113  1110 11:48:19     1\n",
              "2          0  6.150421e+06  5.125563e+06   2.70  116  1110 11:38:19     1\n",
              "3          0  6.149612e+06  5.125907e+06   3.29   95  1110 11:28:19     1\n",
              "4          0  6.148803e+06  5.126252e+06   3.18  108  1110 11:18:19     1\n",
              "...      ...           ...           ...    ...  ...            ...   ...\n",
              "2699633  999  6.138413e+06  5.162715e+06   0.32   40  1031 13:09:00     1\n",
              "2699634  999  6.138412e+06  5.162606e+06   0.22  275  1031 12:48:58     1\n",
              "2699635  999  6.138413e+06  5.162715e+06   0.32    0  1031 12:28:01     1\n",
              "2699636  999  6.138413e+06  5.162715e+06   0.32    0  1031 12:18:00     1\n",
              "2699637  999  6.138413e+06  5.162715e+06   0.11  294  1031 12:07:59     1\n",
              "\n",
              "[2699638 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJsXwfl9JMwq"
      },
      "source": [
        "def feature_engineer(df, test=False):\n",
        "    df['speed'] = df['speed']\n",
        "    df['ori'] = df['ori'] / 180.0 * np.pi\n",
        "    df['speed_sin'] = df['speed'] * np.sin(df['ori'])\n",
        "    df['speed_cos'] = df['speed'] * np.cos(df['ori'])\n",
        "    \n",
        "    if test:\n",
        "        df = df.groupby(['id']).agg({'x': ['std', 'min', 'max', 'mean'], \n",
        "                                     'y': ['std', 'min', 'max', 'mean'], \n",
        "                                     'speed_sin': ['std', 'min', 'max', 'mean'], \n",
        "                                     'speed_cos': ['std', 'min', 'max', 'mean'], \n",
        "                                     'speed': ['std', 'min', 'max', 'mean'], \n",
        "                                     'ori': ['std', 'min', 'max', 'mean']}).reset_index()\n",
        "\n",
        "        df.columns = ['id', \n",
        "                      'x_std', 'x_min', 'x_max', 'x_mean',\n",
        "                      'y_std', 'y_min', 'y_max', 'y_mean', \n",
        "                      'speed_sin_std', 'speed_sin_min', 'speed_sin_max', 'speed_sin_mean', \n",
        "                      'speed_cos_std', 'speed_cos_min', 'speed_cos_max', 'speed_cos_mean',\n",
        "                      'speed_std', 'speed_min', 'speed_max', 'speed_mean', \n",
        "                      'ori_std', 'ori_min', 'ori_max', 'ori_mean']\n",
        "        \n",
        "    else:\n",
        "        df = df.groupby(['id', 'type']).agg({'x': ['std', 'min', 'max', 'mean'], \n",
        "                                             'y': ['std', 'min', 'max', 'mean'], \n",
        "                                             'speed_sin': ['std', 'min', 'max', 'mean'], \n",
        "                                             'speed_cos': ['std', 'min', 'max', 'mean'],\n",
        "                                             'speed': ['std', 'min', 'max', 'mean'], \n",
        "                                             'ori': ['std', 'min', 'max', 'mean']}).reset_index()\n",
        "        df.columns = ['id', 'type', \n",
        "                      'x_std', 'x_min', 'x_max', 'x_mean',\n",
        "                      'y_std', 'y_min', 'y_max', 'y_mean', \n",
        "                      'speed_sin_std', 'speed_sin_min', 'speed_sin_max', 'speed_sin_mean', \n",
        "                      'speed_cos_std', 'speed_cos_min', 'speed_cos_max', 'speed_cos_mean',\n",
        "                      'speed_std', 'speed_min', 'speed_max', 'speed_mean', \n",
        "                      'ori_std', 'ori_min', 'ori_max', 'ori_mean']\n",
        "    \n",
        "    \n",
        "    \n",
        "    return df"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        },
        "id": "Zmcg0CP-JQSy",
        "outputId": "4bb33817-d2e1-4999-cb67-e6c1e7444110"
      },
      "source": [
        "train = feature_engineer(train)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2898\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'speed'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-b1efa820c1a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_engineer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-7d48bce2fd6e>\u001b[0m in \u001b[0;36mfeature_engineer\u001b[0;34m(df, test)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfeature_engineer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'speed'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'speed'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ori'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ori'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m180.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'speed_sin'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'speed'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ori'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'speed_cos'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'speed'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ori'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2904\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2905\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2906\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2907\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2908\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2898\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2900\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2902\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'speed'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "FU7iyEEDk0bB",
        "outputId": "3dcf5cc8-45f8-4137-dbba-9bc185cfb949"
      },
      "source": [
        "train"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>type</th>\n",
              "      <th>x_std</th>\n",
              "      <th>x_min</th>\n",
              "      <th>x_max</th>\n",
              "      <th>x_mean</th>\n",
              "      <th>y_std</th>\n",
              "      <th>y_min</th>\n",
              "      <th>y_max</th>\n",
              "      <th>y_mean</th>\n",
              "      <th>speed_sin_std</th>\n",
              "      <th>speed_sin_min</th>\n",
              "      <th>speed_sin_max</th>\n",
              "      <th>speed_sin_mean</th>\n",
              "      <th>speed_cos_std</th>\n",
              "      <th>speed_cos_min</th>\n",
              "      <th>speed_cos_max</th>\n",
              "      <th>speed_cos_mean</th>\n",
              "      <th>speed_std</th>\n",
              "      <th>speed_min</th>\n",
              "      <th>speed_max</th>\n",
              "      <th>speed_mean</th>\n",
              "      <th>ori_std</th>\n",
              "      <th>ori_min</th>\n",
              "      <th>ori_max</th>\n",
              "      <th>ori_mean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.561751</td>\n",
              "      <td>-0.481411</td>\n",
              "      <td>-0.583659</td>\n",
              "      <td>-0.589869</td>\n",
              "      <td>-0.733104</td>\n",
              "      <td>-0.451055</td>\n",
              "      <td>-0.635067</td>\n",
              "      <td>-0.557333</td>\n",
              "      <td>-0.465947</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.575714</td>\n",
              "      <td>0.489416</td>\n",
              "      <td>-1.374942</td>\n",
              "      <td>-2.571011</td>\n",
              "      <td>-1.135842</td>\n",
              "      <td>-0.132880</td>\n",
              "      <td>-0.493257</td>\n",
              "      <td>0.00</td>\n",
              "      <td>-0.069170</td>\n",
              "      <td>-1.105644</td>\n",
              "      <td>-2.971593</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-3.894416</td>\n",
              "      <td>-2.174746</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.074971</td>\n",
              "      <td>-0.737770</td>\n",
              "      <td>-0.764905</td>\n",
              "      <td>-0.693781</td>\n",
              "      <td>0.547405</td>\n",
              "      <td>-0.774836</td>\n",
              "      <td>-0.704157</td>\n",
              "      <td>-0.702666</td>\n",
              "      <td>0.153581</td>\n",
              "      <td>-7.423240</td>\n",
              "      <td>-0.407276</td>\n",
              "      <td>-0.243683</td>\n",
              "      <td>0.257997</td>\n",
              "      <td>-10.341097</td>\n",
              "      <td>-0.695451</td>\n",
              "      <td>-0.728231</td>\n",
              "      <td>0.581405</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.092406</td>\n",
              "      <td>-0.148014</td>\n",
              "      <td>-0.397695</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.166976</td>\n",
              "      <td>-1.178604</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.766067</td>\n",
              "      <td>-0.242729</td>\n",
              "      <td>-0.469796</td>\n",
              "      <td>-0.352696</td>\n",
              "      <td>-0.774045</td>\n",
              "      <td>-0.179835</td>\n",
              "      <td>-0.392333</td>\n",
              "      <td>-0.305347</td>\n",
              "      <td>0.991019</td>\n",
              "      <td>-1.399147</td>\n",
              "      <td>8.387629</td>\n",
              "      <td>0.543785</td>\n",
              "      <td>-0.001446</td>\n",
              "      <td>-2.101316</td>\n",
              "      <td>3.580308</td>\n",
              "      <td>0.361676</td>\n",
              "      <td>1.569120</td>\n",
              "      <td>0.00</td>\n",
              "      <td>6.075185</td>\n",
              "      <td>-0.870736</td>\n",
              "      <td>0.762649</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.265191</td>\n",
              "      <td>0.120273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.034344</td>\n",
              "      <td>-3.792945</td>\n",
              "      <td>-3.742437</td>\n",
              "      <td>-3.869174</td>\n",
              "      <td>-0.202379</td>\n",
              "      <td>-2.612089</td>\n",
              "      <td>-2.649613</td>\n",
              "      <td>-2.666767</td>\n",
              "      <td>0.443121</td>\n",
              "      <td>-8.190891</td>\n",
              "      <td>0.488293</td>\n",
              "      <td>0.156091</td>\n",
              "      <td>-0.011480</td>\n",
              "      <td>-10.051605</td>\n",
              "      <td>0.645021</td>\n",
              "      <td>0.069020</td>\n",
              "      <td>0.695528</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.035555</td>\n",
              "      <td>-0.245478</td>\n",
              "      <td>0.713557</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.121135</td>\n",
              "      <td>0.077329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.521887</td>\n",
              "      <td>2.983762</td>\n",
              "      <td>2.774408</td>\n",
              "      <td>2.922148</td>\n",
              "      <td>-0.179424</td>\n",
              "      <td>3.378767</td>\n",
              "      <td>3.243342</td>\n",
              "      <td>3.374298</td>\n",
              "      <td>0.131911</td>\n",
              "      <td>-8.392506</td>\n",
              "      <td>0.505638</td>\n",
              "      <td>-0.144988</td>\n",
              "      <td>0.268948</td>\n",
              "      <td>-7.979050</td>\n",
              "      <td>0.621621</td>\n",
              "      <td>0.029078</td>\n",
              "      <td>0.664260</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.035555</td>\n",
              "      <td>-0.287669</td>\n",
              "      <td>0.690527</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.247184</td>\n",
              "      <td>0.423932</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6995</th>\n",
              "      <td>6995</td>\n",
              "      <td>1</td>\n",
              "      <td>0.954782</td>\n",
              "      <td>-0.051338</td>\n",
              "      <td>0.078406</td>\n",
              "      <td>0.048052</td>\n",
              "      <td>0.885207</td>\n",
              "      <td>-0.350640</td>\n",
              "      <td>-0.170606</td>\n",
              "      <td>-0.248711</td>\n",
              "      <td>0.442434</td>\n",
              "      <td>-7.575690</td>\n",
              "      <td>0.591762</td>\n",
              "      <td>1.213694</td>\n",
              "      <td>1.455428</td>\n",
              "      <td>-10.034726</td>\n",
              "      <td>-0.221410</td>\n",
              "      <td>-0.599728</td>\n",
              "      <td>0.964440</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.035555</td>\n",
              "      <td>0.906308</td>\n",
              "      <td>0.822867</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.265191</td>\n",
              "      <td>0.588547</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6996</th>\n",
              "      <td>6996</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.226792</td>\n",
              "      <td>0.686989</td>\n",
              "      <td>0.667973</td>\n",
              "      <td>0.692415</td>\n",
              "      <td>-0.023549</td>\n",
              "      <td>0.763154</td>\n",
              "      <td>0.769158</td>\n",
              "      <td>0.760091</td>\n",
              "      <td>0.107118</td>\n",
              "      <td>-3.354990</td>\n",
              "      <td>0.186597</td>\n",
              "      <td>0.545167</td>\n",
              "      <td>0.806734</td>\n",
              "      <td>-6.584538</td>\n",
              "      <td>0.074428</td>\n",
              "      <td>-0.189717</td>\n",
              "      <td>-0.463305</td>\n",
              "      <td>0.27</td>\n",
              "      <td>-0.149957</td>\n",
              "      <td>0.864612</td>\n",
              "      <td>0.122281</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.229177</td>\n",
              "      <td>0.702050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6997</th>\n",
              "      <td>6997</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.007499</td>\n",
              "      <td>-3.792534</td>\n",
              "      <td>-3.743690</td>\n",
              "      <td>-3.865215</td>\n",
              "      <td>-0.181948</td>\n",
              "      <td>-2.611080</td>\n",
              "      <td>-2.649613</td>\n",
              "      <td>-2.669614</td>\n",
              "      <td>0.560050</td>\n",
              "      <td>-8.364989</td>\n",
              "      <td>0.487543</td>\n",
              "      <td>0.117786</td>\n",
              "      <td>-0.245987</td>\n",
              "      <td>-5.573586</td>\n",
              "      <td>0.290478</td>\n",
              "      <td>0.155278</td>\n",
              "      <td>0.637974</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.035555</td>\n",
              "      <td>-0.224255</td>\n",
              "      <td>0.579324</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.157149</td>\n",
              "      <td>0.395363</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6998</th>\n",
              "      <td>6998</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.416964</td>\n",
              "      <td>0.436643</td>\n",
              "      <td>0.277411</td>\n",
              "      <td>0.356727</td>\n",
              "      <td>0.460642</td>\n",
              "      <td>0.412054</td>\n",
              "      <td>0.493816</td>\n",
              "      <td>0.499432</td>\n",
              "      <td>-0.274385</td>\n",
              "      <td>-7.500917</td>\n",
              "      <td>0.458726</td>\n",
              "      <td>0.096619</td>\n",
              "      <td>0.523549</td>\n",
              "      <td>-5.568184</td>\n",
              "      <td>0.579499</td>\n",
              "      <td>-0.737121</td>\n",
              "      <td>0.207946</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.004138</td>\n",
              "      <td>0.102791</td>\n",
              "      <td>0.296266</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.265191</td>\n",
              "      <td>0.393043</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6999</th>\n",
              "      <td>6999</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.543992</td>\n",
              "      <td>-0.153814</td>\n",
              "      <td>-0.260999</td>\n",
              "      <td>-0.157408</td>\n",
              "      <td>0.267429</td>\n",
              "      <td>-0.422811</td>\n",
              "      <td>-0.253074</td>\n",
              "      <td>-0.187750</td>\n",
              "      <td>-0.859283</td>\n",
              "      <td>-6.349843</td>\n",
              "      <td>0.191737</td>\n",
              "      <td>0.386051</td>\n",
              "      <td>0.524158</td>\n",
              "      <td>-3.196016</td>\n",
              "      <td>0.637523</td>\n",
              "      <td>1.326763</td>\n",
              "      <td>0.626070</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.035555</td>\n",
              "      <td>-0.610770</td>\n",
              "      <td>0.368789</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.265191</td>\n",
              "      <td>-0.648687</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7000 rows × 26 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        id  type     x_std     x_min  ...   ori_std  ori_min   ori_max  ori_mean\n",
              "0        0     1 -0.561751 -0.481411  ... -2.971593      0.0 -3.894416 -2.174746\n",
              "1        1     1 -0.074971 -0.737770  ... -0.397695      0.0 -0.166976 -1.178604\n",
              "2        2     1 -0.766067 -0.242729  ...  0.762649      0.0  0.265191  0.120273\n",
              "3        3     1 -0.034344 -3.792945  ...  0.713557      0.0  0.121135  0.077329\n",
              "4        4     0 -0.521887  2.983762  ...  0.690527      0.0  0.247184  0.423932\n",
              "...    ...   ...       ...       ...  ...       ...      ...       ...       ...\n",
              "6995  6995     1  0.954782 -0.051338  ...  0.822867      0.0  0.265191  0.588547\n",
              "6996  6996     0 -0.226792  0.686989  ...  0.122281      0.0  0.229177  0.702050\n",
              "6997  6997     1 -0.007499 -3.792534  ...  0.579324      0.0  0.157149  0.395363\n",
              "6998  6998     1 -0.416964  0.436643  ...  0.296266      0.0  0.265191  0.393043\n",
              "6999  6999     1 -0.543992 -0.153814  ...  0.368789      0.0  0.265191 -0.648687\n",
              "\n",
              "[7000 rows x 26 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "c8MSGUPiJb86",
        "outputId": "4800638a-5d7d-4626-cd8c-be23216164ca"
      },
      "source": [
        "target = train.type\n",
        "target.hist()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7efcf0fe6450>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PALVSGioJj_D",
        "outputId": "5bb2854c-abb6-4923-cbb5-136f779ecd30"
      },
      "source": [
        "important = ['x_std', 'x_min', 'x_max', 'x_mean', \n",
        "            'y_std', 'y_min', 'y_max', 'y_mean', \n",
        "            'speed_sin_std', 'speed_sin_max', 'speed_sin_mean', \n",
        "            'speed_cos_std', 'speed_cos_max', 'speed_cos_mean',\n",
        "            'speed_std', 'speed_max', 'speed_mean', \n",
        "            'ori_std', 'ori_max', 'ori_mean']\n",
        "\n",
        "num = ['id', \n",
        "       'x_std', 'x_min', 'x_max', 'x_mean', \n",
        "       'y_std', 'y_min', 'y_max', 'y_mean', \n",
        "       'speed_sin_std', 'speed_sin_min', 'speed_sin_max', 'speed_sin_mean', \n",
        "       'speed_cos_std', 'speed_cos_min', 'speed_cos_max', 'speed_cos_mean',\n",
        "       'speed_std', 'speed_min', 'speed_max', 'speed_mean', \n",
        "       'ori_std', 'ori_min', 'ori_max', 'ori_mean']\n",
        "\n",
        "features = [i for i in num if i in important]\n",
        "len(features)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "2Vmidka9Jphb",
        "outputId": "bb825c19-d4fb-4bd1-8537-0a6c2d0d44ee"
      },
      "source": [
        "scaler = StandardScaler()\n",
        "train[important] = scaler.fit_transform(train[important])\n",
        "train[important]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>x_std</th>\n",
              "      <th>x_min</th>\n",
              "      <th>x_max</th>\n",
              "      <th>x_mean</th>\n",
              "      <th>y_std</th>\n",
              "      <th>y_min</th>\n",
              "      <th>y_max</th>\n",
              "      <th>y_mean</th>\n",
              "      <th>speed_sin_std</th>\n",
              "      <th>speed_sin_max</th>\n",
              "      <th>speed_sin_mean</th>\n",
              "      <th>speed_cos_std</th>\n",
              "      <th>speed_cos_max</th>\n",
              "      <th>speed_cos_mean</th>\n",
              "      <th>speed_std</th>\n",
              "      <th>speed_max</th>\n",
              "      <th>speed_mean</th>\n",
              "      <th>ori_std</th>\n",
              "      <th>ori_max</th>\n",
              "      <th>ori_mean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.561751</td>\n",
              "      <td>-0.481411</td>\n",
              "      <td>-0.583659</td>\n",
              "      <td>-0.589869</td>\n",
              "      <td>-0.733104</td>\n",
              "      <td>-0.451055</td>\n",
              "      <td>-0.635067</td>\n",
              "      <td>-0.557333</td>\n",
              "      <td>-0.465947</td>\n",
              "      <td>0.575714</td>\n",
              "      <td>0.489416</td>\n",
              "      <td>-1.374942</td>\n",
              "      <td>-1.135842</td>\n",
              "      <td>-0.132880</td>\n",
              "      <td>-0.493257</td>\n",
              "      <td>-0.069170</td>\n",
              "      <td>-1.105644</td>\n",
              "      <td>-2.971593</td>\n",
              "      <td>-3.894416</td>\n",
              "      <td>-2.174746</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.074971</td>\n",
              "      <td>-0.737770</td>\n",
              "      <td>-0.764905</td>\n",
              "      <td>-0.693781</td>\n",
              "      <td>0.547405</td>\n",
              "      <td>-0.774836</td>\n",
              "      <td>-0.704157</td>\n",
              "      <td>-0.702666</td>\n",
              "      <td>0.153581</td>\n",
              "      <td>-0.407276</td>\n",
              "      <td>-0.243683</td>\n",
              "      <td>0.257997</td>\n",
              "      <td>-0.695451</td>\n",
              "      <td>-0.728231</td>\n",
              "      <td>0.581405</td>\n",
              "      <td>0.092406</td>\n",
              "      <td>-0.148014</td>\n",
              "      <td>-0.397695</td>\n",
              "      <td>-0.166976</td>\n",
              "      <td>-1.178604</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.766067</td>\n",
              "      <td>-0.242729</td>\n",
              "      <td>-0.469796</td>\n",
              "      <td>-0.352696</td>\n",
              "      <td>-0.774045</td>\n",
              "      <td>-0.179835</td>\n",
              "      <td>-0.392333</td>\n",
              "      <td>-0.305347</td>\n",
              "      <td>0.991019</td>\n",
              "      <td>8.387629</td>\n",
              "      <td>0.543785</td>\n",
              "      <td>-0.001446</td>\n",
              "      <td>3.580308</td>\n",
              "      <td>0.361676</td>\n",
              "      <td>1.569120</td>\n",
              "      <td>6.075185</td>\n",
              "      <td>-0.870736</td>\n",
              "      <td>0.762649</td>\n",
              "      <td>0.265191</td>\n",
              "      <td>0.120273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.034344</td>\n",
              "      <td>-3.792945</td>\n",
              "      <td>-3.742437</td>\n",
              "      <td>-3.869174</td>\n",
              "      <td>-0.202379</td>\n",
              "      <td>-2.612089</td>\n",
              "      <td>-2.649613</td>\n",
              "      <td>-2.666767</td>\n",
              "      <td>0.443121</td>\n",
              "      <td>0.488293</td>\n",
              "      <td>0.156091</td>\n",
              "      <td>-0.011480</td>\n",
              "      <td>0.645021</td>\n",
              "      <td>0.069020</td>\n",
              "      <td>0.695528</td>\n",
              "      <td>0.035555</td>\n",
              "      <td>-0.245478</td>\n",
              "      <td>0.713557</td>\n",
              "      <td>0.121135</td>\n",
              "      <td>0.077329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.521887</td>\n",
              "      <td>2.983762</td>\n",
              "      <td>2.774408</td>\n",
              "      <td>2.922148</td>\n",
              "      <td>-0.179424</td>\n",
              "      <td>3.378767</td>\n",
              "      <td>3.243342</td>\n",
              "      <td>3.374298</td>\n",
              "      <td>0.131911</td>\n",
              "      <td>0.505638</td>\n",
              "      <td>-0.144988</td>\n",
              "      <td>0.268948</td>\n",
              "      <td>0.621621</td>\n",
              "      <td>0.029078</td>\n",
              "      <td>0.664260</td>\n",
              "      <td>0.035555</td>\n",
              "      <td>-0.287669</td>\n",
              "      <td>0.690527</td>\n",
              "      <td>0.247184</td>\n",
              "      <td>0.423932</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6995</th>\n",
              "      <td>0.954782</td>\n",
              "      <td>-0.051338</td>\n",
              "      <td>0.078406</td>\n",
              "      <td>0.048052</td>\n",
              "      <td>0.885207</td>\n",
              "      <td>-0.350640</td>\n",
              "      <td>-0.170606</td>\n",
              "      <td>-0.248711</td>\n",
              "      <td>0.442434</td>\n",
              "      <td>0.591762</td>\n",
              "      <td>1.213694</td>\n",
              "      <td>1.455428</td>\n",
              "      <td>-0.221410</td>\n",
              "      <td>-0.599728</td>\n",
              "      <td>0.964440</td>\n",
              "      <td>0.035555</td>\n",
              "      <td>0.906308</td>\n",
              "      <td>0.822867</td>\n",
              "      <td>0.265191</td>\n",
              "      <td>0.588547</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6996</th>\n",
              "      <td>-0.226792</td>\n",
              "      <td>0.686989</td>\n",
              "      <td>0.667973</td>\n",
              "      <td>0.692415</td>\n",
              "      <td>-0.023549</td>\n",
              "      <td>0.763154</td>\n",
              "      <td>0.769158</td>\n",
              "      <td>0.760091</td>\n",
              "      <td>0.107118</td>\n",
              "      <td>0.186597</td>\n",
              "      <td>0.545167</td>\n",
              "      <td>0.806734</td>\n",
              "      <td>0.074428</td>\n",
              "      <td>-0.189717</td>\n",
              "      <td>-0.463305</td>\n",
              "      <td>-0.149957</td>\n",
              "      <td>0.864612</td>\n",
              "      <td>0.122281</td>\n",
              "      <td>0.229177</td>\n",
              "      <td>0.702050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6997</th>\n",
              "      <td>-0.007499</td>\n",
              "      <td>-3.792534</td>\n",
              "      <td>-3.743690</td>\n",
              "      <td>-3.865215</td>\n",
              "      <td>-0.181948</td>\n",
              "      <td>-2.611080</td>\n",
              "      <td>-2.649613</td>\n",
              "      <td>-2.669614</td>\n",
              "      <td>0.560050</td>\n",
              "      <td>0.487543</td>\n",
              "      <td>0.117786</td>\n",
              "      <td>-0.245987</td>\n",
              "      <td>0.290478</td>\n",
              "      <td>0.155278</td>\n",
              "      <td>0.637974</td>\n",
              "      <td>0.035555</td>\n",
              "      <td>-0.224255</td>\n",
              "      <td>0.579324</td>\n",
              "      <td>0.157149</td>\n",
              "      <td>0.395363</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6998</th>\n",
              "      <td>-0.416964</td>\n",
              "      <td>0.436643</td>\n",
              "      <td>0.277411</td>\n",
              "      <td>0.356727</td>\n",
              "      <td>0.460642</td>\n",
              "      <td>0.412054</td>\n",
              "      <td>0.493816</td>\n",
              "      <td>0.499432</td>\n",
              "      <td>-0.274385</td>\n",
              "      <td>0.458726</td>\n",
              "      <td>0.096619</td>\n",
              "      <td>0.523549</td>\n",
              "      <td>0.579499</td>\n",
              "      <td>-0.737121</td>\n",
              "      <td>0.207946</td>\n",
              "      <td>0.004138</td>\n",
              "      <td>0.102791</td>\n",
              "      <td>0.296266</td>\n",
              "      <td>0.265191</td>\n",
              "      <td>0.393043</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6999</th>\n",
              "      <td>-0.543992</td>\n",
              "      <td>-0.153814</td>\n",
              "      <td>-0.260999</td>\n",
              "      <td>-0.157408</td>\n",
              "      <td>0.267429</td>\n",
              "      <td>-0.422811</td>\n",
              "      <td>-0.253074</td>\n",
              "      <td>-0.187750</td>\n",
              "      <td>-0.859283</td>\n",
              "      <td>0.191737</td>\n",
              "      <td>0.386051</td>\n",
              "      <td>0.524158</td>\n",
              "      <td>0.637523</td>\n",
              "      <td>1.326763</td>\n",
              "      <td>0.626070</td>\n",
              "      <td>0.035555</td>\n",
              "      <td>-0.610770</td>\n",
              "      <td>0.368789</td>\n",
              "      <td>0.265191</td>\n",
              "      <td>-0.648687</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7000 rows × 20 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         x_std     x_min     x_max  ...   ori_std   ori_max  ori_mean\n",
              "0    -0.561751 -0.481411 -0.583659  ... -2.971593 -3.894416 -2.174746\n",
              "1    -0.074971 -0.737770 -0.764905  ... -0.397695 -0.166976 -1.178604\n",
              "2    -0.766067 -0.242729 -0.469796  ...  0.762649  0.265191  0.120273\n",
              "3    -0.034344 -3.792945 -3.742437  ...  0.713557  0.121135  0.077329\n",
              "4    -0.521887  2.983762  2.774408  ...  0.690527  0.247184  0.423932\n",
              "...        ...       ...       ...  ...       ...       ...       ...\n",
              "6995  0.954782 -0.051338  0.078406  ...  0.822867  0.265191  0.588547\n",
              "6996 -0.226792  0.686989  0.667973  ...  0.122281  0.229177  0.702050\n",
              "6997 -0.007499 -3.792534 -3.743690  ...  0.579324  0.157149  0.395363\n",
              "6998 -0.416964  0.436643  0.277411  ...  0.296266  0.265191  0.393043\n",
              "6999 -0.543992 -0.153814 -0.260999  ...  0.368789  0.265191 -0.648687\n",
              "\n",
              "[7000 rows x 20 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXtcRNcsJ1SK"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def cm_plot(original_label, predict_label, pic=None):\n",
        "    cm = confusion_matrix(original_label, predict_label)   # 由原标签和预测标签生成混淆矩阵\n",
        "    plt.figure()\n",
        "    plt.matshow(cm, cmap=plt.cm.Blues)     # 画混淆矩阵，配色风格使用cm.Blues\n",
        "    plt.colorbar()    # 颜色标签\n",
        "    for x in range(len(cm)):\n",
        "        for y in range(len(cm)):\n",
        "            plt.annotate(cm[x, y], xy=(x, y), horizontalalignment='center', verticalalignment='center')\n",
        "    plt.xlabel('True label')  # 坐标轴标签\n",
        "    plt.ylabel('Predicted label')  # 坐标轴标签\n",
        "    plt.title('confusion matrix')\n",
        "    if pic is not None:\n",
        "        plt.savefig(str(pic) + '.jpg')\n",
        "    plt.show()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtWxoTg5J4gB"
      },
      "source": [
        "def F1_score(y_true, y_pred):\n",
        "    C=confusion_matrix(y_true, y_pred)\n",
        "    \n",
        "    TP_0 = C[0][0]\n",
        "    FN_0 = C[0][1] + C[0][2]\n",
        "    FP_0 = C[1][0] + C[2][0]\n",
        "    TN_0 = sum(sum(C)) - TP_0 - FN_0 - FP_0\n",
        "    precision_0 = TP_0/(TP_0 + FP_0)\n",
        "    recall_0 = TP_0/(TP_0 + FN_0)\n",
        "    F1_0 = 2 * precision_0 * recall_0/(precision_0 + recall_0)\n",
        "    \n",
        "    TP_1 = C[1][1]\n",
        "    FN_1 = C[1][0] + C[1][2]\n",
        "    FP_1 = C[0][1] + C[2][1]\n",
        "    TN_1 = sum(sum(C)) - TP_1 - FN_1 - FP_1\n",
        "    precision_1 = TP_1/(TP_1 + FP_1)\n",
        "    recall_1 = TP_1/(TP_1 + FN_1)\n",
        "    F1_1 = 2 * precision_1 * recall_1/(precision_1 + recall_1)\n",
        "    \n",
        "    TP_2 = C[2][2]\n",
        "    FN_2 = C[2][0] + C[2][1]\n",
        "    FP_2 = C[0][2] + C[1][2]\n",
        "    TN_2 = sum(sum(C)) - TP_2 - FN_2 - FP_2\n",
        "    precision_2 = TP_2/(TP_2 + FP_2)\n",
        "    recall_2 = TP_2/(TP_2 + FN_2)\n",
        "    F1_2 = 2 * precision_2 * recall_2/(precision_2 + recall_2)\n",
        "    return F1_0, F1_1, F1_2"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQgf8JCzJ825",
        "outputId": "acf9860d-1eb8-479d-a13c-c05364198337"
      },
      "source": [
        "!pip install keras"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.7.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuTRhasHK-3K"
      },
      "source": [
        "import keras.utils"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbNCsm8gLXlJ",
        "outputId": "fdd45453-d43c-407f-eed3-c2a62b19dec3"
      },
      "source": [
        "help(keras.utils)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on package keras.utils in keras:\n",
            "\n",
            "NAME\n",
            "    keras.utils\n",
            "\n",
            "PACKAGE CONTENTS\n",
            "    all_utils\n",
            "    control_flow_util\n",
            "    conv_utils\n",
            "    data_utils\n",
            "    dataset_creator\n",
            "    generic_utils\n",
            "    io_utils\n",
            "    kernelized_utils\n",
            "    kpl_test_utils\n",
            "    layer_utils\n",
            "    losses_utils\n",
            "    metrics_utils\n",
            "    mode_keys\n",
            "    multi_gpu_utils\n",
            "    np_utils\n",
            "    object_identity\n",
            "    tf_contextlib\n",
            "    tf_inspect\n",
            "    tf_utils\n",
            "    traceback_utils\n",
            "    version_utils\n",
            "    vis_utils\n",
            "\n",
            "FILE\n",
            "    /usr/local/lib/python3.7/dist-packages/keras/utils/__init__.py\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lgp4IhmnLnbS"
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "target = to_categorical(target)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXksWo6zMBS6",
        "outputId": "f982d9fc-6f0d-4a80-ea88-4224dcdcde03"
      },
      "source": [
        "target"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       ...,\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czK15NFrMFci"
      },
      "source": [
        "def NN_model():\n",
        "  init = keras.initializers.glorot_uniform(seed=1)\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(Dense(units=32, input_dim=len(important), kernel_initializer=init, activation='relu'))\n",
        "  model.add(Dense(units=64, kernel_initializer=init, activation='relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(units=32, kernel_initializer=init, activation='relu'))\n",
        "  model.add(Dense(units=16, kernel_initializer=init, activation='relu'))\n",
        "  model.add(Dense(units=8, kernel_initializer=init, activation='relu'))\n",
        "  model.add(Dense(units=3, kernel_initializer=init, activation='softmax'))\n",
        "\n",
        "  return model"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rle_Z9uMXbE"
      },
      "source": [
        "n_splits = 5\n",
        "kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=2021)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRWu00rwMbya",
        "outputId": "efe472ee-cfa4-4a78-ecb7-40ac09352f51"
      },
      "source": [
        "import keras \n",
        "\n",
        "\n",
        "b_size = 2\n",
        "max_epochs = 100\n",
        "oof_pred = np.zeros((len(train), ))\n",
        "models = []\n",
        "for fold, (trn_idx, val_idx) in enumerate(kf.split(train[important], train.type)):\n",
        "    print('fold:', fold)\n",
        "    X_train, y_train = train[important].loc[trn_idx], target[trn_idx]\n",
        "    X_val, y_val = train[important].loc[val_idx], target[val_idx]\n",
        "    \n",
        "    model = NN_model()\n",
        "    simple_adam = tensorflow.optimizers.Adam()\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=simple_adam, metrics=['accuracy'])\n",
        "    es = EarlyStopping(monitor='val_score', patience=10, verbose=1, mode='max', restore_best_weights=True,)\n",
        "    es.set_model(model)\n",
        "    metric = Metric(model, [es], [(X_train, y_train), (X_val, y_val)])\n",
        "    model.fit(X_train, y_train, batch_size=b_size, epochs=max_epochs, \n",
        "              validation_data = [X_val, y_val],\n",
        "              callbacks=[metric], shuffle=True, verbose=1)\n",
        "    y_pred3 = model.predict(X_val)\n",
        "    y_pred = np.zeros((len(y_pred3), ))\n",
        "    models.append(model)\n",
        "    for i in range(len(y_pred3)):\n",
        "        y_pred[i] = list(y_pred3[i]).index(max(y_pred3[i]))\n",
        "        \n",
        "    oof_pred[val_idx] = y_pred\n",
        "    F1_0_val, F1_1_val, F1_2_val = F1_score(train.type.loc[val_idx], y_pred)\n",
        "    score_ = (F1_0_val + F1_1_val + F1_2_val)/3\n",
        "    print()\n",
        "    print('F1_0_val is:{}, F1_1_val is:{}, F1_2_val is:{}, total score is : {}'.format(F1_0_val, F1_1_val, F1_2_val, score_))\n",
        "    print()\n",
        "\n",
        "F1_0, F1_1, F1_2 = F1_score(train.type, oof_pred)\n",
        "score_ = (F1_0 + F1_1 + F1_2)/3\n",
        "print()\n",
        "print('F1_0_val is:{}, F1_1_val is:{}, F1_2_val is:{}, total score is : {}'.format(F1_0, F1_1, F1_2, score_))\n",
        "f1_score(train.type, oof_pred, average='macro')"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold: 0\n",
            "Epoch 1/100\n",
            "2787/2800 [============================>.] - ETA: 0s - loss: 0.6799 - accuracy: 0.7131trn_score 0.5723008976449241 val_score 0.5702045423439122\n",
            "2800/2800 [==============================] - 14s 4ms/step - loss: 0.6801 - accuracy: 0.7129 - val_loss: 0.6146 - val_accuracy: 0.7457 - trn_score: 0.5723 - val_score: 0.5702\n",
            "Epoch 2/100\n",
            "2796/2800 [============================>.] - ETA: 0s - loss: 0.5944 - accuracy: 0.7516trn_score 0.7032037713902318 val_score 0.6718760848731725\n",
            "2800/2800 [==============================] - 13s 5ms/step - loss: 0.5940 - accuracy: 0.7518 - val_loss: 0.5879 - val_accuracy: 0.7393 - trn_score: 0.7032 - val_score: 0.6719\n",
            "Epoch 3/100\n",
            "2796/2800 [============================>.] - ETA: 0s - loss: 0.5602 - accuracy: 0.7620trn_score 0.7200259418594536 val_score 0.683788097118868\n",
            "2800/2800 [==============================] - 13s 5ms/step - loss: 0.5602 - accuracy: 0.7620 - val_loss: 0.5733 - val_accuracy: 0.7414 - trn_score: 0.7200 - val_score: 0.6838\n",
            "Epoch 4/100\n",
            "2797/2800 [============================>.] - ETA: 0s - loss: 0.5444 - accuracy: 0.7653trn_score 0.723396698874719 val_score 0.6877758531595705\n",
            "2800/2800 [==============================] - 12s 4ms/step - loss: 0.5446 - accuracy: 0.7652 - val_loss: 0.5664 - val_accuracy: 0.7471 - trn_score: 0.7234 - val_score: 0.6878\n",
            "Epoch 5/100\n",
            "2787/2800 [============================>.] - ETA: 0s - loss: 0.5316 - accuracy: 0.7736trn_score 0.720701152726832 val_score 0.6854590186620548\n",
            "2800/2800 [==============================] - 13s 5ms/step - loss: 0.5312 - accuracy: 0.7736 - val_loss: 0.5631 - val_accuracy: 0.7479 - trn_score: 0.7207 - val_score: 0.6855\n",
            "Epoch 6/100\n",
            "2787/2800 [============================>.] - ETA: 0s - loss: 0.5220 - accuracy: 0.7718trn_score 0.7164294824332327 val_score 0.6858792915399888\n",
            "2800/2800 [==============================] - 13s 5ms/step - loss: 0.5214 - accuracy: 0.7721 - val_loss: 0.5663 - val_accuracy: 0.7657 - trn_score: 0.7164 - val_score: 0.6859\n",
            "Epoch 7/100\n",
            "2799/2800 [============================>.] - ETA: 0s - loss: 0.5119 - accuracy: 0.7792trn_score 0.7244757072388749 val_score 0.6860610664139862\n",
            "2800/2800 [==============================] - 13s 5ms/step - loss: 0.5118 - accuracy: 0.7793 - val_loss: 0.5600 - val_accuracy: 0.7507 - trn_score: 0.7245 - val_score: 0.6861\n",
            "Epoch 8/100\n",
            "2794/2800 [============================>.] - ETA: 0s - loss: 0.5116 - accuracy: 0.7772trn_score 0.7380968624882579 val_score 0.6966274128581693\n",
            "2800/2800 [==============================] - 12s 4ms/step - loss: 0.5114 - accuracy: 0.7773 - val_loss: 0.5528 - val_accuracy: 0.7607 - trn_score: 0.7381 - val_score: 0.6966\n",
            "Epoch 9/100\n",
            "2786/2800 [============================>.] - ETA: 0s - loss: 0.5003 - accuracy: 0.7791trn_score 0.7277465508236395 val_score 0.7013319637574789\n",
            "2800/2800 [==============================] - 13s 5ms/step - loss: 0.5007 - accuracy: 0.7793 - val_loss: 0.5289 - val_accuracy: 0.7714 - trn_score: 0.7277 - val_score: 0.7013\n",
            "Epoch 10/100\n",
            "2794/2800 [============================>.] - ETA: 0s - loss: 0.4903 - accuracy: 0.7845trn_score 0.7262867620318025 val_score 0.689185662225169\n",
            "2800/2800 [==============================] - 13s 5ms/step - loss: 0.4902 - accuracy: 0.7846 - val_loss: 0.5464 - val_accuracy: 0.7664 - trn_score: 0.7263 - val_score: 0.6892\n",
            "Epoch 11/100\n",
            "2791/2800 [============================>.] - ETA: 0s - loss: 0.4892 - accuracy: 0.7834trn_score 0.6758075272992157 val_score 0.6573396979089235\n",
            "2800/2800 [==============================] - 13s 5ms/step - loss: 0.4900 - accuracy: 0.7830 - val_loss: 0.5384 - val_accuracy: 0.7671 - trn_score: 0.6758 - val_score: 0.6573\n",
            "Epoch 12/100\n",
            "2791/2800 [============================>.] - ETA: 0s - loss: 0.4935 - accuracy: 0.7798trn_score 0.6910449817138952 val_score 0.6708576660775556\n",
            "2800/2800 [==============================] - 12s 4ms/step - loss: 0.4928 - accuracy: 0.7804 - val_loss: 0.5322 - val_accuracy: 0.7764 - trn_score: 0.6910 - val_score: 0.6709\n",
            "Epoch 13/100\n",
            "2790/2800 [============================>.] - ETA: 0s - loss: 0.4832 - accuracy: 0.7848trn_score 0.7388842429451686 val_score 0.6987620354747667\n",
            "2800/2800 [==============================] - 12s 4ms/step - loss: 0.4825 - accuracy: 0.7852 - val_loss: 0.5394 - val_accuracy: 0.7664 - trn_score: 0.7389 - val_score: 0.6988\n",
            "Epoch 14/100\n",
            "2794/2800 [============================>.] - ETA: 0s - loss: 0.4862 - accuracy: 0.7863trn_score 0.7178897187887702 val_score 0.6752896930638377\n",
            "2800/2800 [==============================] - 13s 5ms/step - loss: 0.4856 - accuracy: 0.7866 - val_loss: 0.5190 - val_accuracy: 0.7636 - trn_score: 0.7179 - val_score: 0.6753\n",
            "Epoch 15/100\n",
            "2789/2800 [============================>.] - ETA: 0s - loss: 0.4736 - accuracy: 0.7894trn_score 0.7255601639233719 val_score 0.6925594776103489\n",
            "2800/2800 [==============================] - 12s 4ms/step - loss: 0.4739 - accuracy: 0.7889 - val_loss: 0.5247 - val_accuracy: 0.7757 - trn_score: 0.7256 - val_score: 0.6926\n",
            "Epoch 16/100\n",
            "2786/2800 [============================>.] - ETA: 0s - loss: 0.4756 - accuracy: 0.7877trn_score 0.7143873266441302 val_score 0.6763691516005618\n",
            "2800/2800 [==============================] - 12s 4ms/step - loss: 0.4761 - accuracy: 0.7875 - val_loss: 0.5481 - val_accuracy: 0.7693 - trn_score: 0.7144 - val_score: 0.6764\n",
            "Epoch 17/100\n",
            "2791/2800 [============================>.] - ETA: 0s - loss: 0.4792 - accuracy: 0.7872trn_score 0.7081784028860948 val_score 0.6738865413127483\n",
            "2800/2800 [==============================] - 12s 4ms/step - loss: 0.4784 - accuracy: 0.7877 - val_loss: 0.5325 - val_accuracy: 0.7786 - trn_score: 0.7082 - val_score: 0.6739\n",
            "Epoch 18/100\n",
            "2800/2800 [==============================] - ETA: 0s - loss: 0.4705 - accuracy: 0.7945trn_score 0.7479257136299977 val_score 0.6993663299509372\n",
            "2800/2800 [==============================] - 12s 4ms/step - loss: 0.4705 - accuracy: 0.7945 - val_loss: 0.5169 - val_accuracy: 0.7714 - trn_score: 0.7479 - val_score: 0.6994\n",
            "Epoch 19/100\n",
            "2800/2800 [==============================] - ETA: 0s - loss: 0.4684 - accuracy: 0.7914trn_score 0.7443676649572047 val_score 0.7046341902727464\n",
            "2800/2800 [==============================] - 12s 4ms/step - loss: 0.4684 - accuracy: 0.7914 - val_loss: 0.5211 - val_accuracy: 0.7779 - trn_score: 0.7444 - val_score: 0.7046\n",
            "Epoch 20/100\n",
            "2786/2800 [============================>.] - ETA: 0s - loss: 0.4616 - accuracy: 0.7929trn_score 0.745591869724847 val_score 0.7002895633126998\n",
            "2800/2800 [==============================] - 13s 5ms/step - loss: 0.4618 - accuracy: 0.7932 - val_loss: 0.5113 - val_accuracy: 0.7700 - trn_score: 0.7456 - val_score: 0.7003\n",
            "Epoch 21/100\n",
            "2792/2800 [============================>.] - ETA: 0s - loss: 0.4648 - accuracy: 0.7942trn_score 0.7240004791957864 val_score 0.6872034472255658\n",
            "2800/2800 [==============================] - 13s 5ms/step - loss: 0.4660 - accuracy: 0.7937 - val_loss: 0.5203 - val_accuracy: 0.7743 - trn_score: 0.7240 - val_score: 0.6872\n",
            "Epoch 22/100\n",
            "2791/2800 [============================>.] - ETA: 0s - loss: 0.4644 - accuracy: 0.7902trn_score 0.7265864541529323 val_score 0.6962648738847967\n",
            "2800/2800 [==============================] - 12s 4ms/step - loss: 0.4639 - accuracy: 0.7904 - val_loss: 0.5318 - val_accuracy: 0.7764 - trn_score: 0.7266 - val_score: 0.6963\n",
            "Epoch 23/100\n",
            "2794/2800 [============================>.] - ETA: 0s - loss: 0.4667 - accuracy: 0.7929trn_score 0.753599710357896 val_score 0.7072452014057854\n",
            "2800/2800 [==============================] - 12s 4ms/step - loss: 0.4665 - accuracy: 0.7930 - val_loss: 0.5166 - val_accuracy: 0.7743 - trn_score: 0.7536 - val_score: 0.7072\n",
            "Epoch 24/100\n",
            "2790/2800 [============================>.] - ETA: 0s - loss: 0.4548 - accuracy: 0.7968trn_score 0.7320992900496174 val_score 0.6986513673847036\n",
            "2800/2800 [==============================] - 13s 5ms/step - loss: 0.4550 - accuracy: 0.7968 - val_loss: 0.4997 - val_accuracy: 0.7779 - trn_score: 0.7321 - val_score: 0.6987\n",
            "Epoch 25/100\n",
            "2788/2800 [============================>.] - ETA: 0s - loss: 0.4534 - accuracy: 0.7936trn_score 0.7495250721975721 val_score 0.6949659103609802\n",
            "2800/2800 [==============================] - 13s 5ms/step - loss: 0.4549 - accuracy: 0.7934 - val_loss: 0.5441 - val_accuracy: 0.7679 - trn_score: 0.7495 - val_score: 0.6950\n",
            "Epoch 26/100\n",
            "2799/2800 [============================>.] - ETA: 0s - loss: 0.4492 - accuracy: 0.8023trn_score 0.7415133606336646 val_score 0.7013025133080485\n",
            "2800/2800 [==============================] - 13s 5ms/step - loss: 0.4494 - accuracy: 0.8021 - val_loss: 0.5456 - val_accuracy: 0.7657 - trn_score: 0.7415 - val_score: 0.7013\n",
            "Epoch 27/100\n",
            "2791/2800 [============================>.] - ETA: 0s - loss: 0.4475 - accuracy: 0.7995trn_score 0.7555850002820841 val_score 0.7166388441446214\n",
            "2800/2800 [==============================] - 13s 5ms/step - loss: 0.4473 - accuracy: 0.7995 - val_loss: 0.5452 - val_accuracy: 0.7864 - trn_score: 0.7556 - val_score: 0.7166\n",
            "Epoch 28/100\n",
            "2795/2800 [============================>.] - ETA: 0s - loss: 0.4524 - accuracy: 0.7970trn_score 0.7192528954050214 val_score 0.6827880167535612\n",
            "2800/2800 [==============================] - 12s 4ms/step - loss: 0.4517 - accuracy: 0.7973 - val_loss: 0.5434 - val_accuracy: 0.7771 - trn_score: 0.7193 - val_score: 0.6828\n",
            "Epoch 29/100\n",
            "2792/2800 [============================>.] - ETA: 0s - loss: 0.4520 - accuracy: 0.8012trn_score 0.7548804757468841 val_score 0.713380813318235\n",
            "2800/2800 [==============================] - 12s 4ms/step - loss: 0.4527 - accuracy: 0.8014 - val_loss: 0.5432 - val_accuracy: 0.7807 - trn_score: 0.7549 - val_score: 0.7134\n",
            "Epoch 30/100\n",
            "2790/2800 [============================>.] - ETA: 0s - loss: 0.4497 - accuracy: 0.7995trn_score 0.7631582674008692 val_score 0.7288333689612863\n",
            "2800/2800 [==============================] - 12s 4ms/step - loss: 0.4495 - accuracy: 0.7996 - val_loss: 0.5428 - val_accuracy: 0.7914 - trn_score: 0.7632 - val_score: 0.7288\n",
            "Epoch 31/100\n",
            "2793/2800 [============================>.] - ETA: 0s - loss: 0.4475 - accuracy: 0.7948trn_score 0.7673829195474108 val_score 0.7232984269191244\n",
            "2800/2800 [==============================] - 12s 4ms/step - loss: 0.4483 - accuracy: 0.7943 - val_loss: 0.5281 - val_accuracy: 0.7829 - trn_score: 0.7674 - val_score: 0.7233\n",
            "Epoch 32/100\n",
            "2786/2800 [============================>.] - ETA: 0s - loss: 0.4410 - accuracy: 0.7992trn_score 0.7604928637869836 val_score 0.7209661832983997\n",
            "2800/2800 [==============================] - 13s 5ms/step - loss: 0.4422 - accuracy: 0.7984 - val_loss: 0.5264 - val_accuracy: 0.7800 - trn_score: 0.7605 - val_score: 0.7210\n",
            "Epoch 33/100\n",
            "2794/2800 [============================>.] - ETA: 0s - loss: 0.4514 - accuracy: 0.7949trn_score 0.7390996740828153 val_score 0.6999201613251259\n",
            "2800/2800 [==============================] - 13s 5ms/step - loss: 0.4520 - accuracy: 0.7945 - val_loss: 0.5483 - val_accuracy: 0.7814 - trn_score: 0.7391 - val_score: 0.6999\n",
            "Epoch 34/100\n",
            "2795/2800 [============================>.] - ETA: 0s - loss: 0.4386 - accuracy: 0.8038trn_score 0.7675081488101756 val_score 0.718097173003847\n",
            "2800/2800 [==============================] - 13s 5ms/step - loss: 0.4396 - accuracy: 0.8032 - val_loss: 0.5164 - val_accuracy: 0.7821 - trn_score: 0.7675 - val_score: 0.7181\n",
            "Epoch 35/100\n",
            "2793/2800 [============================>.] - ETA: 0s - loss: 0.4318 - accuracy: 0.8081trn_score 0.7511641013233875 val_score 0.6981379373207671\n",
            "2800/2800 [==============================] - 13s 5ms/step - loss: 0.4322 - accuracy: 0.8079 - val_loss: 0.5128 - val_accuracy: 0.7793 - trn_score: 0.7512 - val_score: 0.6981\n",
            "Epoch 36/100\n",
            "2799/2800 [============================>.] - ETA: 0s - loss: 0.4394 - accuracy: 0.8021trn_score 0.7601404157175464 val_score 0.7280669479872932\n",
            "2800/2800 [==============================] - 13s 5ms/step - loss: 0.4395 - accuracy: 0.8021 - val_loss: 0.5307 - val_accuracy: 0.7914 - trn_score: 0.7601 - val_score: 0.7281\n",
            "Epoch 37/100\n",
            "2794/2800 [============================>.] - ETA: 0s - loss: 0.4364 - accuracy: 0.7990trn_score 0.7713362017777016 val_score 0.7231833652978158\n",
            "2800/2800 [==============================] - 12s 4ms/step - loss: 0.4364 - accuracy: 0.7991 - val_loss: 0.5180 - val_accuracy: 0.7857 - trn_score: 0.7713 - val_score: 0.7232\n",
            "Epoch 38/100\n",
            "2794/2800 [============================>.] - ETA: 0s - loss: 0.4360 - accuracy: 0.8026trn_score 0.7696941922517505 val_score 0.7228798210669765\n",
            "2800/2800 [==============================] - 13s 5ms/step - loss: 0.4360 - accuracy: 0.8025 - val_loss: 0.5488 - val_accuracy: 0.7843 - trn_score: 0.7697 - val_score: 0.7229\n",
            "Epoch 39/100\n",
            "2787/2800 [============================>.] - ETA: 0s - loss: 0.4327 - accuracy: 0.8093trn_score 0.7682288259310956 val_score 0.7298342796506812\n",
            "2800/2800 [==============================] - 13s 5ms/step - loss: 0.4324 - accuracy: 0.8095 - val_loss: 0.5278 - val_accuracy: 0.7893 - trn_score: 0.7682 - val_score: 0.7298\n",
            "Epoch 40/100\n",
            "2794/2800 [============================>.] - ETA: 0s - loss: 0.4332 - accuracy: 0.8105trn_score 0.7584487123901749 val_score 0.7224420776062362\n",
            "2800/2800 [==============================] - 12s 4ms/step - loss: 0.4343 - accuracy: 0.8098 - val_loss: 0.5171 - val_accuracy: 0.7864 - trn_score: 0.7584 - val_score: 0.7224\n",
            "Epoch 41/100\n",
            "2791/2800 [============================>.] - ETA: 0s - loss: 0.4303 - accuracy: 0.8072trn_score 0.7504022757190315 val_score 0.6946766833621233\n",
            "2800/2800 [==============================] - 13s 5ms/step - loss: 0.4302 - accuracy: 0.8071 - val_loss: 0.5664 - val_accuracy: 0.7779 - trn_score: 0.7504 - val_score: 0.6947\n",
            "Epoch 42/100\n",
            "2796/2800 [============================>.] - ETA: 0s - loss: 0.4247 - accuracy: 0.8053trn_score 0.760813901503376 val_score 0.7225925302300213\n",
            "2800/2800 [==============================] - 13s 5ms/step - loss: 0.4248 - accuracy: 0.8052 - val_loss: 0.5841 - val_accuracy: 0.7864 - trn_score: 0.7608 - val_score: 0.7226\n",
            "Epoch 43/100\n",
            "2796/2800 [============================>.] - ETA: 0s - loss: 0.4351 - accuracy: 0.8094trn_score 0.7714448861511557 val_score 0.7176026016135882\n",
            "2800/2800 [==============================] - 12s 4ms/step - loss: 0.4357 - accuracy: 0.8089 - val_loss: 0.5274 - val_accuracy: 0.7800 - trn_score: 0.7714 - val_score: 0.7176\n",
            "Epoch 44/100\n",
            "2795/2800 [============================>.] - ETA: 0s - loss: 0.4165 - accuracy: 0.8116trn_score 0.7703002972279757 val_score 0.723849482081021\n",
            "2800/2800 [==============================] - 12s 4ms/step - loss: 0.4171 - accuracy: 0.8114 - val_loss: 0.5839 - val_accuracy: 0.7914 - trn_score: 0.7703 - val_score: 0.7238\n",
            "Epoch 45/100\n",
            "2789/2800 [============================>.] - ETA: 0s - loss: 0.4298 - accuracy: 0.8037trn_score 0.7729679041206122 val_score 0.7235293698227334\n",
            "2800/2800 [==============================] - 12s 4ms/step - loss: 0.4295 - accuracy: 0.8037 - val_loss: 0.5357 - val_accuracy: 0.7864 - trn_score: 0.7730 - val_score: 0.7235\n",
            "Epoch 46/100\n",
            "2800/2800 [==============================] - ETA: 0s - loss: 0.4281 - accuracy: 0.8029trn_score 0.7548210438388384 val_score 0.7112360252509428\n",
            "2800/2800 [==============================] - 12s 4ms/step - loss: 0.4281 - accuracy: 0.8029 - val_loss: 0.5681 - val_accuracy: 0.7893 - trn_score: 0.7548 - val_score: 0.7112\n",
            "Epoch 47/100\n",
            "2793/2800 [============================>.] - ETA: 0s - loss: 0.4269 - accuracy: 0.8090trn_score 0.7861822747861574 val_score 0.7255029868563025\n",
            "2800/2800 [==============================] - 13s 5ms/step - loss: 0.4269 - accuracy: 0.8089 - val_loss: 0.5458 - val_accuracy: 0.7879 - trn_score: 0.7862 - val_score: 0.7255\n",
            "Epoch 48/100\n",
            "2789/2800 [============================>.] - ETA: 0s - loss: 0.4233 - accuracy: 0.8078trn_score 0.7660487230172843 val_score 0.7223157633815909\n",
            "2800/2800 [==============================] - 12s 4ms/step - loss: 0.4236 - accuracy: 0.8075 - val_loss: 0.5749 - val_accuracy: 0.7864 - trn_score: 0.7660 - val_score: 0.7223\n",
            "Epoch 49/100\n",
            "2796/2800 [============================>.] - ETA: 0s - loss: 0.4225 - accuracy: 0.8119trn_score 0.7685950461016996 val_score 0.7202757045921664\n",
            "Restoring model weights from the end of the best epoch: 39.\n",
            "2800/2800 [==============================] - 13s 5ms/step - loss: 0.4223 - accuracy: 0.8118 - val_loss: 0.5627 - val_accuracy: 0.7843 - trn_score: 0.7686 - val_score: 0.7203\n",
            "Epoch 00049: early stopping\n",
            "\n",
            "F1_0_val is:0.7510668563300142, F1_1_val is:0.8574836016696481, F1_2_val is:0.5809523809523811, total score is : 0.7298342796506812\n",
            "\n",
            "fold: 1\n",
            "Epoch 1/100\n",
            "   2/2800 [..............................] - ETA: 6:53:11 - loss: 1.2392 - accuracy: 0.2500  WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0050s vs `on_train_batch_end` time: 1.4768s). Check your callbacks.\n",
            "2792/2800 [============================>.] - ETA: 0s - loss: 0.7220 - accuracy: 0.6981trn_score 0.6740795696225587 val_score 0.6696582700989399\n",
            "2800/2800 [==============================] - 23s 8ms/step - loss: 0.7212 - accuracy: 0.6986 - val_loss: 0.6030 - val_accuracy: 0.7429 - trn_score: 0.6741 - val_score: 0.6697\n",
            "Epoch 2/100\n",
            "2790/2800 [============================>.] - ETA: 0s - loss: 0.6001 - accuracy: 0.7339trn_score 0.6253976917557867 val_score 0.6292561061110641\n",
            "2800/2800 [==============================] - 12s 4ms/step - loss: 0.5998 - accuracy: 0.7337 - val_loss: 0.5657 - val_accuracy: 0.7643 - trn_score: 0.6254 - val_score: 0.6293\n",
            "Epoch 3/100\n",
            "2796/2800 [============================>.] - ETA: 0s - loss: 0.5652 - accuracy: 0.7432trn_score 0.6316666118376726 val_score 0.6418859879200353\n",
            "2800/2800 [==============================] - 12s 4ms/step - loss: 0.5657 - accuracy: 0.7427 - val_loss: 0.5417 - val_accuracy: 0.7757 - trn_score: 0.6317 - val_score: 0.6419\n",
            "Epoch 4/100\n",
            "2787/2800 [============================>.] - ETA: 0s - loss: 0.5464 - accuracy: 0.7530trn_score 0.7055317834109216 val_score 0.7048191199570418\n",
            "2800/2800 [==============================] - 13s 5ms/step - loss: 0.5473 - accuracy: 0.7523 - val_loss: 0.5341 - val_accuracy: 0.7729 - trn_score: 0.7055 - val_score: 0.7048\n",
            "Epoch 5/100\n",
            "2795/2800 [============================>.] - ETA: 0s - loss: 0.5293 - accuracy: 0.7567trn_score 0.7268201115515432 val_score 0.7234147709079699\n",
            "2800/2800 [==============================] - 13s 5ms/step - loss: 0.5299 - accuracy: 0.7564 - val_loss: 0.5160 - val_accuracy: 0.7814 - trn_score: 0.7268 - val_score: 0.7234\n",
            "Epoch 6/100\n",
            "2791/2800 [============================>.] - ETA: 0s - loss: 0.5208 - accuracy: 0.7700trn_score 0.7051934894157764 val_score 0.7068706958392461\n",
            "2800/2800 [==============================] - 13s 5ms/step - loss: 0.5200 - accuracy: 0.7700 - val_loss: 0.5274 - val_accuracy: 0.7743 - trn_score: 0.7052 - val_score: 0.7069\n",
            "Epoch 7/100\n",
            "2793/2800 [============================>.] - ETA: 0s - loss: 0.5167 - accuracy: 0.7666trn_score 0.6894817944153963 val_score 0.6736596502624912\n",
            "2800/2800 [==============================] - 12s 4ms/step - loss: 0.5163 - accuracy: 0.7668 - val_loss: 0.5118 - val_accuracy: 0.7793 - trn_score: 0.6895 - val_score: 0.6737\n",
            "Epoch 8/100\n",
            "2800/2800 [==============================] - ETA: 0s - loss: 0.5030 - accuracy: 0.7793trn_score 0.7010292479114382 val_score 0.7095368931528651\n",
            "2800/2800 [==============================] - 13s 5ms/step - loss: 0.5030 - accuracy: 0.7793 - val_loss: 0.5226 - val_accuracy: 0.7843 - trn_score: 0.7010 - val_score: 0.7095\n",
            "Epoch 9/100\n",
            "2793/2800 [============================>.] - ETA: 0s - loss: 0.4951 - accuracy: 0.7728trn_score 0.7121710443132852 val_score 0.698955860473514\n",
            "2800/2800 [==============================] - 12s 4ms/step - loss: 0.4955 - accuracy: 0.7727 - val_loss: 0.5096 - val_accuracy: 0.7821 - trn_score: 0.7122 - val_score: 0.6990\n",
            "Epoch 10/100\n",
            "2796/2800 [============================>.] - ETA: 0s - loss: 0.4947 - accuracy: 0.7772trn_score 0.7237649766744481 val_score 0.7388509574741132\n",
            "2800/2800 [==============================] - 12s 4ms/step - loss: 0.4949 - accuracy: 0.7768 - val_loss: 0.4865 - val_accuracy: 0.7979 - trn_score: 0.7238 - val_score: 0.7389\n",
            "Epoch 11/100\n",
            "2792/2800 [============================>.] - ETA: 0s - loss: 0.4858 - accuracy: 0.7733trn_score 0.7164905323651926 val_score 0.7273467540400836\n",
            "2800/2800 [==============================] - 12s 4ms/step - loss: 0.4860 - accuracy: 0.7732 - val_loss: 0.5044 - val_accuracy: 0.7914 - trn_score: 0.7165 - val_score: 0.7273\n",
            "Epoch 12/100\n",
            "2786/2800 [============================>.] - ETA: 0s - loss: 0.4750 - accuracy: 0.7800trn_score 0.7281519688630539 val_score 0.7213939406921863\n",
            "2800/2800 [==============================] - 12s 4ms/step - loss: 0.4756 - accuracy: 0.7798 - val_loss: 0.4825 - val_accuracy: 0.7936 - trn_score: 0.7282 - val_score: 0.7214\n",
            "Epoch 13/100\n",
            "2799/2800 [============================>.] - ETA: 0s - loss: 0.4796 - accuracy: 0.7755trn_score 0.7341819819641738 val_score 0.7403611405574071\n",
            "2800/2800 [==============================] - 12s 4ms/step - loss: 0.4794 - accuracy: 0.7755 - val_loss: 0.4761 - val_accuracy: 0.7979 - trn_score: 0.7342 - val_score: 0.7404\n",
            "Epoch 14/100\n",
            "2788/2800 [============================>.] - ETA: 0s - loss: 0.4775 - accuracy: 0.7794trn_score 0.7377291388093217 val_score 0.7335931685834631\n",
            "2800/2800 [==============================] - 12s 4ms/step - loss: 0.4771 - accuracy: 0.7796 - val_loss: 0.4858 - val_accuracy: 0.7936 - trn_score: 0.7377 - val_score: 0.7336\n",
            "Epoch 15/100\n",
            "2794/2800 [============================>.] - ETA: 0s - loss: 0.4707 - accuracy: 0.7815trn_score 0.6587503994523785 val_score 0.6199291352554965\n",
            "2800/2800 [==============================] - 13s 5ms/step - loss: 0.4708 - accuracy: 0.7816 - val_loss: 0.4886 - val_accuracy: 0.7664 - trn_score: 0.6588 - val_score: 0.6199\n",
            "Epoch 16/100\n",
            "2794/2800 [============================>.] - ETA: 0s - loss: 0.4680 - accuracy: 0.7817trn_score 0.7141410604802085 val_score 0.6867520838370004\n",
            "2800/2800 [==============================] - 12s 4ms/step - loss: 0.4685 - accuracy: 0.7816 - val_loss: 0.4865 - val_accuracy: 0.7757 - trn_score: 0.7141 - val_score: 0.6868\n",
            "Epoch 17/100\n",
            "2799/2800 [============================>.] - ETA: 0s - loss: 0.4636 - accuracy: 0.7855trn_score 0.7414492853295765 val_score 0.7200548349465651\n",
            "2800/2800 [==============================] - 12s 4ms/step - loss: 0.4635 - accuracy: 0.7855 - val_loss: 0.4730 - val_accuracy: 0.7929 - trn_score: 0.7414 - val_score: 0.7201\n",
            "Epoch 18/100\n",
            "2788/2800 [============================>.] - ETA: 0s - loss: 0.4616 - accuracy: 0.7826trn_score 0.7286594850805267 val_score 0.7302344974002004\n",
            "2800/2800 [==============================] - 12s 4ms/step - loss: 0.4624 - accuracy: 0.7818 - val_loss: 0.4866 - val_accuracy: 0.7971 - trn_score: 0.7287 - val_score: 0.7302\n",
            "Epoch 19/100\n",
            "2795/2800 [============================>.] - ETA: 0s - loss: 0.4705 - accuracy: 0.7832trn_score 0.7334723195713121 val_score 0.7170386198324668\n",
            "2800/2800 [==============================] - 12s 4ms/step - loss: 0.4703 - accuracy: 0.7834 - val_loss: 0.4612 - val_accuracy: 0.7964 - trn_score: 0.7335 - val_score: 0.7170\n",
            "Epoch 20/100\n",
            "2799/2800 [============================>.] - ETA: 0s - loss: 0.4579 - accuracy: 0.7812trn_score 0.7381345833039923 val_score 0.734545111338893\n",
            "2800/2800 [==============================] - 13s 5ms/step - loss: 0.4580 - accuracy: 0.7811 - val_loss: 0.4685 - val_accuracy: 0.7971 - trn_score: 0.7381 - val_score: 0.7345\n",
            "Epoch 21/100\n",
            "2797/2800 [============================>.] - ETA: 0s - loss: 0.4569 - accuracy: 0.7855trn_score 0.7541341717147806 val_score 0.7560181251770036\n",
            "2800/2800 [==============================] - 13s 5ms/step - loss: 0.4570 - accuracy: 0.7854 - val_loss: 0.4879 - val_accuracy: 0.8093 - trn_score: 0.7541 - val_score: 0.7560\n",
            "Epoch 22/100\n",
            "2790/2800 [============================>.] - ETA: 0s - loss: 0.4530 - accuracy: 0.7909trn_score 0.7507954569870248 val_score 0.7417753223669169\n",
            "2800/2800 [==============================] - 13s 5ms/step - loss: 0.4540 - accuracy: 0.7905 - val_loss: 0.4685 - val_accuracy: 0.8021 - trn_score: 0.7508 - val_score: 0.7418\n",
            "Epoch 23/100\n",
            "2790/2800 [============================>.] - ETA: 0s - loss: 0.4501 - accuracy: 0.7821trn_score 0.7230333031532302 val_score 0.6923223743139341\n",
            "2800/2800 [==============================] - 13s 5ms/step - loss: 0.4502 - accuracy: 0.7821 - val_loss: 0.5132 - val_accuracy: 0.7857 - trn_score: 0.7230 - val_score: 0.6923\n",
            "Epoch 24/100\n",
            "2798/2800 [============================>.] - ETA: 0s - loss: 0.4550 - accuracy: 0.7906trn_score 0.736166770044581 val_score 0.7227356121621206\n",
            "2800/2800 [==============================] - 12s 4ms/step - loss: 0.4552 - accuracy: 0.7902 - val_loss: 0.4851 - val_accuracy: 0.7907 - trn_score: 0.7362 - val_score: 0.7227\n",
            "Epoch 25/100\n",
            "2794/2800 [============================>.] - ETA: 0s - loss: 0.4471 - accuracy: 0.7872trn_score 0.7427377028377847 val_score 0.7134606459468444\n",
            "2800/2800 [==============================] - 12s 4ms/step - loss: 0.4471 - accuracy: 0.7870 - val_loss: 0.4733 - val_accuracy: 0.7857 - trn_score: 0.7427 - val_score: 0.7135\n",
            "Epoch 26/100\n",
            "2796/2800 [============================>.] - ETA: 0s - loss: 0.4503 - accuracy: 0.7913trn_score 0.7267899915520091 val_score 0.6998160201881617\n",
            "2800/2800 [==============================] - 12s 4ms/step - loss: 0.4503 - accuracy: 0.7912 - val_loss: 0.4793 - val_accuracy: 0.7936 - trn_score: 0.7268 - val_score: 0.6998\n",
            "Epoch 27/100\n",
            "2791/2800 [============================>.] - ETA: 0s - loss: 0.4400 - accuracy: 0.8004trn_score 0.758347897964995 val_score 0.7353101407071757\n",
            "2800/2800 [==============================] - 14s 5ms/step - loss: 0.4398 - accuracy: 0.8007 - val_loss: 0.4735 - val_accuracy: 0.7907 - trn_score: 0.7583 - val_score: 0.7353\n",
            "Epoch 28/100\n",
            "2786/2800 [============================>.] - ETA: 0s - loss: 0.4464 - accuracy: 0.7907trn_score 0.7448839153526642 val_score 0.7250739275362948\n",
            "2800/2800 [==============================] - 12s 4ms/step - loss: 0.4454 - accuracy: 0.7911 - val_loss: 0.4749 - val_accuracy: 0.8000 - trn_score: 0.7449 - val_score: 0.7251\n",
            "Epoch 29/100\n",
            "2795/2800 [============================>.] - ETA: 0s - loss: 0.4465 - accuracy: 0.7961trn_score 0.7556556579024453 val_score 0.741747730021629\n",
            "2800/2800 [==============================] - 12s 4ms/step - loss: 0.4463 - accuracy: 0.7962 - val_loss: 0.4757 - val_accuracy: 0.8014 - trn_score: 0.7557 - val_score: 0.7417\n",
            "Epoch 30/100\n",
            "2786/2800 [============================>.] - ETA: 0s - loss: 0.4466 - accuracy: 0.7931trn_score 0.7486459392939281 val_score 0.7328347568217691\n",
            "2800/2800 [==============================] - 13s 5ms/step - loss: 0.4465 - accuracy: 0.7932 - val_loss: 0.4845 - val_accuracy: 0.8057 - trn_score: 0.7486 - val_score: 0.7328\n",
            "Epoch 31/100\n",
            "2794/2800 [============================>.] - ETA: 0s - loss: 0.4407 - accuracy: 0.7953trn_score 0.7590437793420813 val_score 0.7385927893584604\n",
            "Restoring model weights from the end of the best epoch: 21.\n",
            "2800/2800 [==============================] - 14s 5ms/step - loss: 0.4404 - accuracy: 0.7955 - val_loss: 0.4750 - val_accuracy: 0.7921 - trn_score: 0.7590 - val_score: 0.7386\n",
            "Epoch 00031: early stopping\n",
            "\n",
            "F1_0_val is:0.7727272727272727, F1_1_val is:0.8691588785046729, F1_2_val is:0.6261682242990654, total score is : 0.7560181251770036\n",
            "\n",
            "fold: 2\n",
            "Epoch 1/100\n",
            "2788/2800 [============================>.] - ETA: 0s - loss: 0.6980 - accuracy: 0.7134trn_score 0.5174622260793418 val_score 0.517659023668639\n",
            "2800/2800 [==============================] - 13s 5ms/step - loss: 0.6972 - accuracy: 0.7138 - val_loss: 0.5904 - val_accuracy: 0.7429 - trn_score: 0.5175 - val_score: 0.5177\n",
            "Epoch 2/100\n",
            "2798/2800 [============================>.] - ETA: 0s - loss: 0.5974 - accuracy: 0.7398trn_score 0.7046114154632991 val_score 0.6846884641702768\n",
            "2800/2800 [==============================] - 12s 4ms/step - loss: 0.5974 - accuracy: 0.7398 - val_loss: 0.5746 - val_accuracy: 0.7507 - trn_score: 0.7046 - val_score: 0.6847\n",
            "Epoch 3/100\n",
            "2795/2800 [============================>.] - ETA: 0s - loss: 0.5683 - accuracy: 0.7581trn_score 0.6655209761896326 val_score 0.633159063081023\n",
            "2800/2800 [==============================] - 14s 5ms/step - loss: 0.5680 - accuracy: 0.7584 - val_loss: 0.5535 - val_accuracy: 0.7486 - trn_score: 0.6655 - val_score: 0.6332\n",
            "Epoch 4/100\n",
            "2796/2800 [============================>.] - ETA: 0s - loss: 0.5485 - accuracy: 0.7597trn_score 0.7170865597950201 val_score 0.7007366111052442\n",
            "2800/2800 [==============================] - 13s 5ms/step - loss: 0.5484 - accuracy: 0.7596 - val_loss: 0.5279 - val_accuracy: 0.7536 - trn_score: 0.7171 - val_score: 0.7007\n",
            "Epoch 5/100\n",
            "2794/2800 [============================>.] - ETA: 0s - loss: 0.5328 - accuracy: 0.7672trn_score 0.6611964717419498 val_score 0.6327592364613238\n",
            "2800/2800 [==============================] - 12s 4ms/step - loss: 0.5329 - accuracy: 0.7673 - val_loss: 0.5003 - val_accuracy: 0.7650 - trn_score: 0.6612 - val_score: 0.6328\n",
            "Epoch 6/100\n",
            "2799/2800 [============================>.] - ETA: 0s - loss: 0.5127 - accuracy: 0.7783trn_score 0.6582986507713421 val_score 0.6396976839776749\n",
            "2800/2800 [==============================] - 13s 4ms/step - loss: 0.5127 - accuracy: 0.7782 - val_loss: 0.5088 - val_accuracy: 0.7621 - trn_score: 0.6583 - val_score: 0.6397\n",
            "Epoch 7/100\n",
            "2791/2800 [============================>.] - ETA: 0s - loss: 0.5148 - accuracy: 0.7732trn_score 0.7328703127123012 val_score 0.7151580904631528\n",
            "2800/2800 [==============================] - 13s 4ms/step - loss: 0.5140 - accuracy: 0.7736 - val_loss: 0.5000 - val_accuracy: 0.7650 - trn_score: 0.7329 - val_score: 0.7152\n",
            "Epoch 8/100\n",
            "2798/2800 [============================>.] - ETA: 0s - loss: 0.5062 - accuracy: 0.7772trn_score 0.6841566333392471 val_score 0.6510024590550273\n",
            "2800/2800 [==============================] - 13s 4ms/step - loss: 0.5058 - accuracy: 0.7773 - val_loss: 0.5037 - val_accuracy: 0.7621 - trn_score: 0.6842 - val_score: 0.6510\n",
            "Epoch 9/100\n",
            "2789/2800 [============================>.] - ETA: 0s - loss: 0.5051 - accuracy: 0.7795trn_score 0.7147552413796191 val_score 0.6967083690494013\n",
            "2800/2800 [==============================] - 14s 5ms/step - loss: 0.5039 - accuracy: 0.7800 - val_loss: 0.4908 - val_accuracy: 0.7771 - trn_score: 0.7148 - val_score: 0.6967\n",
            "Epoch 10/100\n",
            "2800/2800 [==============================] - ETA: 0s - loss: 0.4918 - accuracy: 0.7830trn_score 0.7044079599786106 val_score 0.6790253301486314\n",
            "2800/2800 [==============================] - 14s 5ms/step - loss: 0.4918 - accuracy: 0.7830 - val_loss: 0.4968 - val_accuracy: 0.7736 - trn_score: 0.7044 - val_score: 0.6790\n",
            "Epoch 11/100\n",
            "2797/2800 [============================>.] - ETA: 0s - loss: 0.4865 - accuracy: 0.7846trn_score 0.730319170440677 val_score 0.7316253444820635\n",
            "2800/2800 [==============================] - 13s 4ms/step - loss: 0.4862 - accuracy: 0.7846 - val_loss: 0.4764 - val_accuracy: 0.7929 - trn_score: 0.7303 - val_score: 0.7316\n",
            "Epoch 12/100\n",
            "2795/2800 [============================>.] - ETA: 0s - loss: 0.4909 - accuracy: 0.7871trn_score 0.7146184926540963 val_score 0.6929506836718891\n",
            "2800/2800 [==============================] - 12s 4ms/step - loss: 0.4911 - accuracy: 0.7871 - val_loss: 0.4707 - val_accuracy: 0.7836 - trn_score: 0.7146 - val_score: 0.6930\n",
            "Epoch 13/100\n",
            "2798/2800 [============================>.] - ETA: 0s - loss: 0.4802 - accuracy: 0.7891trn_score 0.7249963187970515 val_score 0.7023796070684871\n",
            "2800/2800 [==============================] - 13s 5ms/step - loss: 0.4801 - accuracy: 0.7891 - val_loss: 0.4751 - val_accuracy: 0.7814 - trn_score: 0.7250 - val_score: 0.7024\n",
            "Epoch 14/100\n",
            "2795/2800 [============================>.] - ETA: 0s - loss: 0.4814 - accuracy: 0.7869trn_score 0.6836920269406784 val_score 0.6621023025847592\n",
            "2800/2800 [==============================] - 14s 5ms/step - loss: 0.4810 - accuracy: 0.7871 - val_loss: 0.4734 - val_accuracy: 0.7871 - trn_score: 0.6837 - val_score: 0.6621\n",
            "Epoch 15/100\n",
            "2790/2800 [============================>.] - ETA: 0s - loss: 0.4720 - accuracy: 0.7882trn_score 0.6946283560168461 val_score 0.6829631799898666\n",
            "2800/2800 [==============================] - 14s 5ms/step - loss: 0.4719 - accuracy: 0.7884 - val_loss: 0.4578 - val_accuracy: 0.7864 - trn_score: 0.6946 - val_score: 0.6830\n",
            "Epoch 16/100\n",
            "2790/2800 [============================>.] - ETA: 0s - loss: 0.4764 - accuracy: 0.7839trn_score 0.7568904911507269 val_score 0.7347704160283145\n",
            "2800/2800 [==============================] - 12s 4ms/step - loss: 0.4762 - accuracy: 0.7839 - val_loss: 0.4612 - val_accuracy: 0.7864 - trn_score: 0.7569 - val_score: 0.7348\n",
            "Epoch 17/100\n",
            "2794/2800 [============================>.] - ETA: 0s - loss: 0.4698 - accuracy: 0.7901trn_score 0.7418876703219947 val_score 0.735625555199085\n",
            "2800/2800 [==============================] - 14s 5ms/step - loss: 0.4693 - accuracy: 0.7904 - val_loss: 0.4675 - val_accuracy: 0.7936 - trn_score: 0.7419 - val_score: 0.7356\n",
            "Epoch 18/100\n",
            "2793/2800 [============================>.] - ETA: 0s - loss: 0.4619 - accuracy: 0.7888trn_score 0.7535736080987149 val_score 0.7387081980031277\n",
            "2800/2800 [==============================] - 13s 5ms/step - loss: 0.4620 - accuracy: 0.7886 - val_loss: 0.4608 - val_accuracy: 0.7964 - trn_score: 0.7536 - val_score: 0.7387\n",
            "Epoch 19/100\n",
            "2800/2800 [==============================] - ETA: 0s - loss: 0.4596 - accuracy: 0.7914trn_score 0.7546780481609877 val_score 0.7431378781646161\n",
            "2800/2800 [==============================] - 14s 5ms/step - loss: 0.4596 - accuracy: 0.7914 - val_loss: 0.4526 - val_accuracy: 0.7957 - trn_score: 0.7547 - val_score: 0.7431\n",
            "Epoch 20/100\n",
            "2796/2800 [============================>.] - ETA: 0s - loss: 0.4569 - accuracy: 0.7967trn_score 0.7411359939993832 val_score 0.7167941395769747\n",
            "2800/2800 [==============================] - 14s 5ms/step - loss: 0.4566 - accuracy: 0.7966 - val_loss: 0.4699 - val_accuracy: 0.7871 - trn_score: 0.7411 - val_score: 0.7168\n",
            "Epoch 21/100\n",
            "2797/2800 [============================>.] - ETA: 0s - loss: 0.4571 - accuracy: 0.7978trn_score 0.7308072872793686 val_score 0.6963687768582089\n",
            "2800/2800 [==============================] - 13s 5ms/step - loss: 0.4574 - accuracy: 0.7977 - val_loss: 0.4602 - val_accuracy: 0.7829 - trn_score: 0.7308 - val_score: 0.6964\n",
            "Epoch 22/100\n",
            "2788/2800 [============================>.] - ETA: 0s - loss: 0.4549 - accuracy: 0.7972trn_score 0.7375662128042887 val_score 0.7135056486687027\n",
            "2800/2800 [==============================] - 14s 5ms/step - loss: 0.4548 - accuracy: 0.7971 - val_loss: 0.4619 - val_accuracy: 0.7900 - trn_score: 0.7376 - val_score: 0.7135\n",
            "Epoch 23/100\n",
            "2791/2800 [============================>.] - ETA: 0s - loss: 0.4427 - accuracy: 0.7994trn_score 0.7523205754286079 val_score 0.7354632552640886\n",
            "2800/2800 [==============================] - 13s 5ms/step - loss: 0.4431 - accuracy: 0.7989 - val_loss: 0.4616 - val_accuracy: 0.8014 - trn_score: 0.7523 - val_score: 0.7355\n",
            "Epoch 24/100\n",
            "2787/2800 [============================>.] - ETA: 0s - loss: 0.4565 - accuracy: 0.7944trn_score 0.7473590526587325 val_score 0.7299603841396879\n",
            "2800/2800 [==============================] - 14s 5ms/step - loss: 0.4571 - accuracy: 0.7939 - val_loss: 0.4547 - val_accuracy: 0.7957 - trn_score: 0.7474 - val_score: 0.7300\n",
            "Epoch 25/100\n",
            "2798/2800 [============================>.] - ETA: 0s - loss: 0.4538 - accuracy: 0.7965trn_score 0.7383101316704126 val_score 0.700424354949896\n",
            "2800/2800 [==============================] - 14s 5ms/step - loss: 0.4539 - accuracy: 0.7964 - val_loss: 0.4602 - val_accuracy: 0.7814 - trn_score: 0.7383 - val_score: 0.7004\n",
            "Epoch 26/100\n",
            "2799/2800 [============================>.] - ETA: 0s - loss: 0.4393 - accuracy: 0.8019trn_score 0.7653131135628329 val_score 0.7441630079927952\n",
            "2800/2800 [==============================] - 13s 5ms/step - loss: 0.4395 - accuracy: 0.8016 - val_loss: 0.4683 - val_accuracy: 0.7900 - trn_score: 0.7653 - val_score: 0.7442\n",
            "Epoch 27/100\n",
            "2790/2800 [============================>.] - ETA: 0s - loss: 0.4401 - accuracy: 0.7977trn_score 0.7618877611958256 val_score 0.7308505870019436\n",
            "2800/2800 [==============================] - 14s 5ms/step - loss: 0.4400 - accuracy: 0.7975 - val_loss: 0.4591 - val_accuracy: 0.7943 - trn_score: 0.7619 - val_score: 0.7309\n",
            "Epoch 28/100\n",
            "2790/2800 [============================>.] - ETA: 0s - loss: 0.4349 - accuracy: 0.7953trn_score 0.7529009849522671 val_score 0.7203118957229653\n",
            "2800/2800 [==============================] - 13s 4ms/step - loss: 0.4342 - accuracy: 0.7959 - val_loss: 0.4558 - val_accuracy: 0.7957 - trn_score: 0.7529 - val_score: 0.7203\n",
            "Epoch 29/100\n",
            "2790/2800 [============================>.] - ETA: 0s - loss: 0.4371 - accuracy: 0.8004trn_score 0.7511801731668183 val_score 0.7141533916853707\n",
            "2800/2800 [==============================] - 13s 5ms/step - loss: 0.4378 - accuracy: 0.8000 - val_loss: 0.4508 - val_accuracy: 0.7907 - trn_score: 0.7512 - val_score: 0.7142\n",
            "Epoch 30/100\n",
            "2795/2800 [============================>.] - ETA: 0s - loss: 0.4252 - accuracy: 0.8054trn_score 0.7622937916601346 val_score 0.7414066931966726\n",
            "2800/2800 [==============================] - 13s 4ms/step - loss: 0.4255 - accuracy: 0.8054 - val_loss: 0.4620 - val_accuracy: 0.7914 - trn_score: 0.7623 - val_score: 0.7414\n",
            "Epoch 31/100\n",
            "2799/2800 [============================>.] - ETA: 0s - loss: 0.4422 - accuracy: 0.8006trn_score 0.720027323736162 val_score 0.6941673424862761\n",
            "2800/2800 [==============================] - 13s 5ms/step - loss: 0.4421 - accuracy: 0.8007 - val_loss: 0.4561 - val_accuracy: 0.7921 - trn_score: 0.7200 - val_score: 0.6942\n",
            "Epoch 32/100\n",
            "2800/2800 [==============================] - ETA: 0s - loss: 0.4354 - accuracy: 0.7961trn_score 0.7450519241574977 val_score 0.7036729383730028\n",
            "2800/2800 [==============================] - 13s 5ms/step - loss: 0.4354 - accuracy: 0.7961 - val_loss: 0.4705 - val_accuracy: 0.7793 - trn_score: 0.7451 - val_score: 0.7037\n",
            "Epoch 33/100\n",
            "2793/2800 [============================>.] - ETA: 0s - loss: 0.4373 - accuracy: 0.7970trn_score 0.7350543397611545 val_score 0.709191693330733\n",
            "2800/2800 [==============================] - 13s 5ms/step - loss: 0.4374 - accuracy: 0.7968 - val_loss: 0.4450 - val_accuracy: 0.7907 - trn_score: 0.7351 - val_score: 0.7092\n",
            "Epoch 34/100\n",
            "2796/2800 [============================>.] - ETA: 0s - loss: 0.4409 - accuracy: 0.8031trn_score 0.7555992348360264 val_score 0.725053675802183\n",
            "2800/2800 [==============================] - 14s 5ms/step - loss: 0.4404 - accuracy: 0.8034 - val_loss: 0.4610 - val_accuracy: 0.7921 - trn_score: 0.7556 - val_score: 0.7251\n",
            "Epoch 35/100\n",
            "2791/2800 [============================>.] - ETA: 0s - loss: 0.4299 - accuracy: 0.8046trn_score 0.7298581165520618 val_score 0.6932947214170824\n",
            "2800/2800 [==============================] - 13s 5ms/step - loss: 0.4301 - accuracy: 0.8045 - val_loss: 0.4572 - val_accuracy: 0.7936 - trn_score: 0.7299 - val_score: 0.6933\n",
            "Epoch 36/100\n",
            "2789/2800 [============================>.] - ETA: 0s - loss: 0.4333 - accuracy: 0.8046trn_score 0.7647542969489649 val_score 0.7402597373643168\n",
            "Restoring model weights from the end of the best epoch: 26.\n",
            "2800/2800 [==============================] - 13s 5ms/step - loss: 0.4336 - accuracy: 0.8041 - val_loss: 0.4533 - val_accuracy: 0.7950 - trn_score: 0.7648 - val_score: 0.7403\n",
            "Epoch 00036: early stopping\n",
            "\n",
            "F1_0_val is:0.7515151515151515, F1_1_val is:0.8547112462006079, F1_2_val is:0.6262626262626263, total score is : 0.7441630079927952\n",
            "\n",
            "fold: 3\n",
            "Epoch 1/100\n",
            "2794/2800 [============================>.] - ETA: 0s - loss: 0.6877 - accuracy: 0.7128trn_score 0.673506346962799 val_score 0.683644832747094\n",
            "2800/2800 [==============================] - 14s 5ms/step - loss: 0.6869 - accuracy: 0.7132 - val_loss: 0.5793 - val_accuracy: 0.7564 - trn_score: 0.6735 - val_score: 0.6836\n",
            "Epoch 2/100\n",
            "2797/2800 [============================>.] - ETA: 0s - loss: 0.5959 - accuracy: 0.7494trn_score 0.666725109788739 val_score 0.6411246596923152\n",
            "2800/2800 [==============================] - 14s 5ms/step - loss: 0.5958 - accuracy: 0.7495 - val_loss: 0.5327 - val_accuracy: 0.7686 - trn_score: 0.6667 - val_score: 0.6411\n",
            "Epoch 3/100\n",
            "2800/2800 [==============================] - ETA: 0s - loss: 0.5681 - accuracy: 0.7580trn_score 0.7050755970035806 val_score 0.7036408485323346\n",
            "2800/2800 [==============================] - 13s 5ms/step - loss: 0.5681 - accuracy: 0.7580 - val_loss: 0.5515 - val_accuracy: 0.7643 - trn_score: 0.7051 - val_score: 0.7036\n",
            "Epoch 4/100\n",
            "2793/2800 [============================>.] - ETA: 0s - loss: 0.5565 - accuracy: 0.7610trn_score 0.7086437904793108 val_score 0.6848117941442179\n",
            "2800/2800 [==============================] - 14s 5ms/step - loss: 0.5563 - accuracy: 0.7611 - val_loss: 0.5124 - val_accuracy: 0.7700 - trn_score: 0.7086 - val_score: 0.6848\n",
            "Epoch 5/100\n",
            "2797/2800 [============================>.] - ETA: 0s - loss: 0.5430 - accuracy: 0.7576trn_score 0.6901148653381471 val_score 0.6693895864337644\n",
            "2800/2800 [==============================] - 13s 5ms/step - loss: 0.5430 - accuracy: 0.7575 - val_loss: 0.5240 - val_accuracy: 0.7714 - trn_score: 0.6901 - val_score: 0.6694\n",
            "Epoch 6/100\n",
            "2792/2800 [============================>.] - ETA: 0s - loss: 0.5278 - accuracy: 0.7654trn_score 0.7109805402929447 val_score 0.6956402248672781\n",
            "2800/2800 [==============================] - 13s 5ms/step - loss: 0.5283 - accuracy: 0.7654 - val_loss: 0.5004 - val_accuracy: 0.7779 - trn_score: 0.7110 - val_score: 0.6956\n",
            "Epoch 7/100\n",
            "2792/2800 [============================>.] - ETA: 0s - loss: 0.5226 - accuracy: 0.7656trn_score 0.7216763167490773 val_score 0.7061200687687729\n",
            "2800/2800 [==============================] - 14s 5ms/step - loss: 0.5221 - accuracy: 0.7659 - val_loss: 0.5006 - val_accuracy: 0.7829 - trn_score: 0.7217 - val_score: 0.7061\n",
            "Epoch 8/100\n",
            "2795/2800 [============================>.] - ETA: 0s - loss: 0.5140 - accuracy: 0.7753trn_score 0.7112166154651053 val_score 0.6959460396726315\n",
            "2800/2800 [==============================] - 13s 5ms/step - loss: 0.5147 - accuracy: 0.7752 - val_loss: 0.4882 - val_accuracy: 0.7843 - trn_score: 0.7112 - val_score: 0.6959\n",
            "Epoch 9/100\n",
            "2789/2800 [============================>.] - ETA: 0s - loss: 0.5104 - accuracy: 0.7770trn_score 0.7271798103286105 val_score 0.693242398151234\n",
            "2800/2800 [==============================] - 14s 5ms/step - loss: 0.5098 - accuracy: 0.7773 - val_loss: 0.4903 - val_accuracy: 0.7686 - trn_score: 0.7272 - val_score: 0.6932\n",
            "Epoch 10/100\n",
            "2796/2800 [============================>.] - ETA: 0s - loss: 0.5017 - accuracy: 0.7808trn_score 0.7187198395106256 val_score 0.6861824118245186\n",
            "2800/2800 [==============================] - 14s 5ms/step - loss: 0.5014 - accuracy: 0.7809 - val_loss: 0.4984 - val_accuracy: 0.7607 - trn_score: 0.7187 - val_score: 0.6862\n",
            "Epoch 11/100\n",
            "2800/2800 [==============================] - ETA: 0s - loss: 0.4969 - accuracy: 0.7761trn_score 0.7426785283483769 val_score 0.714034955925016\n",
            "2800/2800 [==============================] - 13s 5ms/step - loss: 0.4969 - accuracy: 0.7761 - val_loss: 0.4860 - val_accuracy: 0.7771 - trn_score: 0.7427 - val_score: 0.7140\n",
            "Epoch 12/100\n",
            "2796/2800 [============================>.] - ETA: 0s - loss: 0.4897 - accuracy: 0.7854trn_score 0.7144338815893305 val_score 0.6902060438056318\n",
            "2800/2800 [==============================] - 13s 5ms/step - loss: 0.4896 - accuracy: 0.7854 - val_loss: 0.4974 - val_accuracy: 0.7857 - trn_score: 0.7144 - val_score: 0.6902\n",
            "Epoch 13/100\n",
            "2788/2800 [============================>.] - ETA: 0s - loss: 0.4867 - accuracy: 0.7893trn_score 0.739646433697402 val_score 0.7262763753337911\n",
            "2800/2800 [==============================] - 13s 5ms/step - loss: 0.4869 - accuracy: 0.7887 - val_loss: 0.4809 - val_accuracy: 0.7907 - trn_score: 0.7396 - val_score: 0.7263\n",
            "Epoch 14/100\n",
            "2794/2800 [============================>.] - ETA: 0s - loss: 0.4846 - accuracy: 0.7811trn_score 0.7330198095460542 val_score 0.7083132623929588\n",
            "2800/2800 [==============================] - 14s 5ms/step - loss: 0.4843 - accuracy: 0.7812 - val_loss: 0.4745 - val_accuracy: 0.7786 - trn_score: 0.7330 - val_score: 0.7083\n",
            "Epoch 15/100\n",
            "2793/2800 [============================>.] - ETA: 0s - loss: 0.4722 - accuracy: 0.7889trn_score 0.741977987931965 val_score 0.7166259082647196\n",
            "2800/2800 [==============================] - 14s 5ms/step - loss: 0.4717 - accuracy: 0.7893 - val_loss: 0.4651 - val_accuracy: 0.7886 - trn_score: 0.7420 - val_score: 0.7166\n",
            "Epoch 16/100\n",
            "2790/2800 [============================>.] - ETA: 0s - loss: 0.4717 - accuracy: 0.7892trn_score 0.7415033258540914 val_score 0.7172589470584896\n",
            "2800/2800 [==============================] - 14s 5ms/step - loss: 0.4714 - accuracy: 0.7895 - val_loss: 0.4629 - val_accuracy: 0.7886 - trn_score: 0.7415 - val_score: 0.7173\n",
            "Epoch 17/100\n",
            "2788/2800 [============================>.] - ETA: 0s - loss: 0.4667 - accuracy: 0.7889trn_score 0.7445096465394373 val_score 0.706818690442096\n",
            "2800/2800 [==============================] - 14s 5ms/step - loss: 0.4679 - accuracy: 0.7884 - val_loss: 0.4858 - val_accuracy: 0.7807 - trn_score: 0.7445 - val_score: 0.7068\n",
            "Epoch 18/100\n",
            "2796/2800 [============================>.] - ETA: 0s - loss: 0.4707 - accuracy: 0.7895trn_score 0.7571172775086668 val_score 0.730181373000522\n",
            "2800/2800 [==============================] - 15s 5ms/step - loss: 0.4707 - accuracy: 0.7895 - val_loss: 0.4661 - val_accuracy: 0.7900 - trn_score: 0.7571 - val_score: 0.7302\n",
            "Epoch 19/100\n",
            "2794/2800 [============================>.] - ETA: 0s - loss: 0.4631 - accuracy: 0.7931trn_score 0.7057155084366292 val_score 0.6530443971418837\n",
            "2800/2800 [==============================] - 15s 5ms/step - loss: 0.4630 - accuracy: 0.7932 - val_loss: 0.4642 - val_accuracy: 0.7807 - trn_score: 0.7057 - val_score: 0.6530\n",
            "Epoch 20/100\n",
            "2796/2800 [============================>.] - ETA: 0s - loss: 0.4652 - accuracy: 0.7910trn_score 0.7513083586872318 val_score 0.7138118006351556\n",
            "2800/2800 [==============================] - 14s 5ms/step - loss: 0.4649 - accuracy: 0.7911 - val_loss: 0.4646 - val_accuracy: 0.7871 - trn_score: 0.7513 - val_score: 0.7138\n",
            "Epoch 21/100\n",
            "2796/2800 [============================>.] - ETA: 0s - loss: 0.4621 - accuracy: 0.7859trn_score 0.7285769914482202 val_score 0.7132691358451085\n",
            "2800/2800 [==============================] - 13s 5ms/step - loss: 0.4622 - accuracy: 0.7859 - val_loss: 0.4768 - val_accuracy: 0.7907 - trn_score: 0.7286 - val_score: 0.7133\n",
            "Epoch 22/100\n",
            "2798/2800 [============================>.] - ETA: 0s - loss: 0.4535 - accuracy: 0.7924trn_score 0.7403637323963617 val_score 0.7085407124883544\n",
            "2800/2800 [==============================] - 13s 5ms/step - loss: 0.4545 - accuracy: 0.7921 - val_loss: 0.4852 - val_accuracy: 0.7779 - trn_score: 0.7404 - val_score: 0.7085\n",
            "Epoch 23/100\n",
            "2796/2800 [============================>.] - ETA: 0s - loss: 0.4489 - accuracy: 0.7899trn_score 0.7375212736445373 val_score 0.6917348833804521\n",
            "2800/2800 [==============================] - 13s 5ms/step - loss: 0.4488 - accuracy: 0.7900 - val_loss: 0.4768 - val_accuracy: 0.7836 - trn_score: 0.7375 - val_score: 0.6917\n",
            "Epoch 24/100\n",
            "2795/2800 [============================>.] - ETA: 0s - loss: 0.4501 - accuracy: 0.7962trn_score 0.7560417513732292 val_score 0.7182747185604056\n",
            "2800/2800 [==============================] - 13s 5ms/step - loss: 0.4497 - accuracy: 0.7962 - val_loss: 0.4652 - val_accuracy: 0.7950 - trn_score: 0.7560 - val_score: 0.7183\n",
            "Epoch 25/100\n",
            "2790/2800 [============================>.] - ETA: 0s - loss: 0.4511 - accuracy: 0.7984trn_score 0.7084352813048777 val_score 0.6594802982410901\n",
            "2800/2800 [==============================] - 15s 5ms/step - loss: 0.4515 - accuracy: 0.7982 - val_loss: 0.4768 - val_accuracy: 0.7743 - trn_score: 0.7084 - val_score: 0.6595\n",
            "Epoch 26/100\n",
            "2797/2800 [============================>.] - ETA: 0s - loss: 0.4436 - accuracy: 0.7887trn_score 0.7506958795293069 val_score 0.71244101648514\n",
            "2800/2800 [==============================] - 13s 5ms/step - loss: 0.4433 - accuracy: 0.7887 - val_loss: 0.4745 - val_accuracy: 0.7893 - trn_score: 0.7507 - val_score: 0.7124\n",
            "Epoch 27/100\n",
            "2788/2800 [============================>.] - ETA: 0s - loss: 0.4469 - accuracy: 0.7939trn_score 0.7285056187894452 val_score 0.6776798092389668\n",
            "2800/2800 [==============================] - 15s 5ms/step - loss: 0.4464 - accuracy: 0.7943 - val_loss: 0.4719 - val_accuracy: 0.7829 - trn_score: 0.7285 - val_score: 0.6777\n",
            "Epoch 28/100\n",
            "2793/2800 [============================>.] - ETA: 0s - loss: 0.4489 - accuracy: 0.7945trn_score 0.7620369533507158 val_score 0.715687394776051\n",
            "Restoring model weights from the end of the best epoch: 18.\n",
            "2800/2800 [==============================] - 15s 5ms/step - loss: 0.4484 - accuracy: 0.7948 - val_loss: 0.5070 - val_accuracy: 0.7786 - trn_score: 0.7620 - val_score: 0.7157\n",
            "Epoch 00028: early stopping\n",
            "\n",
            "F1_0_val is:0.7484848484848485, F1_1_val is:0.8617021276595745, F1_2_val is:0.5803571428571429, total score is : 0.730181373000522\n",
            "\n",
            "fold: 4\n",
            "Epoch 1/100\n",
            "2799/2800 [============================>.] - ETA: 0s - loss: 0.6862 - accuracy: 0.7201trn_score 0.5808815376032468 val_score 0.5837154710920974\n",
            "2800/2800 [==============================] - 14s 5ms/step - loss: 0.6864 - accuracy: 0.7198 - val_loss: 0.6069 - val_accuracy: 0.7429 - trn_score: 0.5809 - val_score: 0.5837\n",
            "Epoch 2/100\n",
            "2791/2800 [============================>.] - ETA: 0s - loss: 0.6003 - accuracy: 0.7517trn_score 0.5779737617511135 val_score 0.5789504583841395\n",
            "2800/2800 [==============================] - 14s 5ms/step - loss: 0.6012 - accuracy: 0.7511 - val_loss: 0.5620 - val_accuracy: 0.7507 - trn_score: 0.5780 - val_score: 0.5790\n",
            "Epoch 3/100\n",
            "2794/2800 [============================>.] - ETA: 0s - loss: 0.5641 - accuracy: 0.7589trn_score 0.7120337033173646 val_score 0.7071527813351034\n",
            "2800/2800 [==============================] - 14s 5ms/step - loss: 0.5643 - accuracy: 0.7588 - val_loss: 0.5422 - val_accuracy: 0.7586 - trn_score: 0.7120 - val_score: 0.7072\n",
            "Epoch 4/100\n",
            "2789/2800 [============================>.] - ETA: 0s - loss: 0.5453 - accuracy: 0.7644trn_score 0.7141117819328034 val_score 0.7001703399213746\n",
            "2800/2800 [==============================] - 13s 5ms/step - loss: 0.5460 - accuracy: 0.7639 - val_loss: 0.5214 - val_accuracy: 0.7664 - trn_score: 0.7141 - val_score: 0.7002\n",
            "Epoch 5/100\n",
            "2791/2800 [============================>.] - ETA: 0s - loss: 0.5282 - accuracy: 0.7660trn_score 0.6999408474959564 val_score 0.6990466491305664\n",
            "2800/2800 [==============================] - 15s 5ms/step - loss: 0.5280 - accuracy: 0.7664 - val_loss: 0.5182 - val_accuracy: 0.7764 - trn_score: 0.6999 - val_score: 0.6990\n",
            "Epoch 6/100\n",
            "2798/2800 [============================>.] - ETA: 0s - loss: 0.5218 - accuracy: 0.7698trn_score 0.6787566146682967 val_score 0.6975356251551719\n",
            "2800/2800 [==============================] - 15s 5ms/step - loss: 0.5218 - accuracy: 0.7696 - val_loss: 0.5177 - val_accuracy: 0.7850 - trn_score: 0.6788 - val_score: 0.6975\n",
            "Epoch 7/100\n",
            "2790/2800 [============================>.] - ETA: 0s - loss: 0.5121 - accuracy: 0.7744trn_score 0.654749937720032 val_score 0.6387832112570805\n",
            "2800/2800 [==============================] - 15s 5ms/step - loss: 0.5122 - accuracy: 0.7743 - val_loss: 0.5003 - val_accuracy: 0.7643 - trn_score: 0.6547 - val_score: 0.6388\n",
            "Epoch 8/100\n",
            "2791/2800 [============================>.] - ETA: 0s - loss: 0.5021 - accuracy: 0.7757trn_score 0.6563461932630036 val_score 0.6635916927358032\n",
            "2800/2800 [==============================] - 14s 5ms/step - loss: 0.5017 - accuracy: 0.7759 - val_loss: 0.5072 - val_accuracy: 0.7729 - trn_score: 0.6563 - val_score: 0.6636\n",
            "Epoch 9/100\n",
            "2788/2800 [============================>.] - ETA: 0s - loss: 0.5004 - accuracy: 0.7762trn_score 0.6978347780043147 val_score 0.684676331220133\n",
            "2800/2800 [==============================] - 15s 5ms/step - loss: 0.4999 - accuracy: 0.7764 - val_loss: 0.5332 - val_accuracy: 0.7707 - trn_score: 0.6978 - val_score: 0.6847\n",
            "Epoch 10/100\n",
            "2799/2800 [============================>.] - ETA: 0s - loss: 0.4950 - accuracy: 0.7812trn_score 0.6865361672352527 val_score 0.6661888772213177\n",
            "2800/2800 [==============================] - 14s 5ms/step - loss: 0.4949 - accuracy: 0.7812 - val_loss: 0.4878 - val_accuracy: 0.7743 - trn_score: 0.6865 - val_score: 0.6662\n",
            "Epoch 11/100\n",
            "2792/2800 [============================>.] - ETA: 0s - loss: 0.4849 - accuracy: 0.7853trn_score 0.6918797965850833 val_score 0.7026857837679875\n",
            "2800/2800 [==============================] - 15s 5ms/step - loss: 0.4852 - accuracy: 0.7852 - val_loss: 0.4942 - val_accuracy: 0.7879 - trn_score: 0.6919 - val_score: 0.7027\n",
            "Epoch 12/100\n",
            "2798/2800 [============================>.] - ETA: 0s - loss: 0.4820 - accuracy: 0.7847trn_score 0.7046752128231275 val_score 0.685728873352097\n",
            "2800/2800 [==============================] - 13s 5ms/step - loss: 0.4820 - accuracy: 0.7848 - val_loss: 0.4918 - val_accuracy: 0.7779 - trn_score: 0.7047 - val_score: 0.6857\n",
            "Epoch 13/100\n",
            "2788/2800 [============================>.] - ETA: 0s - loss: 0.4756 - accuracy: 0.7889trn_score 0.7440853833049553 val_score 0.7258019353833333\n",
            "2800/2800 [==============================] - 15s 5ms/step - loss: 0.4770 - accuracy: 0.7886 - val_loss: 0.4907 - val_accuracy: 0.7864 - trn_score: 0.7441 - val_score: 0.7258\n",
            "Epoch 14/100\n",
            "2798/2800 [============================>.] - ETA: 0s - loss: 0.4736 - accuracy: 0.7872trn_score 0.7296168202115877 val_score 0.7176365317318657\n",
            "2800/2800 [==============================] - 15s 5ms/step - loss: 0.4736 - accuracy: 0.7871 - val_loss: 0.4964 - val_accuracy: 0.7886 - trn_score: 0.7296 - val_score: 0.7176\n",
            "Epoch 15/100\n",
            "2797/2800 [============================>.] - ETA: 0s - loss: 0.4742 - accuracy: 0.7864trn_score 0.6953953616984755 val_score 0.708647652718919\n",
            "2800/2800 [==============================] - 15s 5ms/step - loss: 0.4741 - accuracy: 0.7864 - val_loss: 0.4881 - val_accuracy: 0.7964 - trn_score: 0.6954 - val_score: 0.7086\n",
            "Epoch 16/100\n",
            "2790/2800 [============================>.] - ETA: 0s - loss: 0.4648 - accuracy: 0.7907trn_score 0.581113269924878 val_score 0.5676728367170385\n",
            "2800/2800 [==============================] - 15s 5ms/step - loss: 0.4646 - accuracy: 0.7907 - val_loss: 0.5039 - val_accuracy: 0.7629 - trn_score: 0.5811 - val_score: 0.5677\n",
            "Epoch 17/100\n",
            "2799/2800 [============================>.] - ETA: 0s - loss: 0.4707 - accuracy: 0.7890trn_score 0.6980193296901872 val_score 0.6963570200276634\n",
            "2800/2800 [==============================] - 14s 5ms/step - loss: 0.4707 - accuracy: 0.7889 - val_loss: 0.5074 - val_accuracy: 0.7821 - trn_score: 0.6980 - val_score: 0.6964\n",
            "Epoch 18/100\n",
            "2799/2800 [============================>.] - ETA: 0s - loss: 0.4663 - accuracy: 0.7919trn_score 0.7476076384918144 val_score 0.7188541844198526\n",
            "2800/2800 [==============================] - 14s 5ms/step - loss: 0.4661 - accuracy: 0.7920 - val_loss: 0.5006 - val_accuracy: 0.7771 - trn_score: 0.7476 - val_score: 0.7189\n",
            "Epoch 19/100\n",
            "2798/2800 [============================>.] - ETA: 0s - loss: 0.4665 - accuracy: 0.7932trn_score 0.693684571261877 val_score 0.6906126486631168\n",
            "2800/2800 [==============================] - 15s 5ms/step - loss: 0.4663 - accuracy: 0.7934 - val_loss: 0.5059 - val_accuracy: 0.7786 - trn_score: 0.6937 - val_score: 0.6906\n",
            "Epoch 20/100\n",
            "2800/2800 [==============================] - ETA: 0s - loss: 0.4606 - accuracy: 0.7879trn_score 0.6918058369427277 val_score 0.6938203319815361\n",
            "2800/2800 [==============================] - 15s 5ms/step - loss: 0.4606 - accuracy: 0.7879 - val_loss: 0.4883 - val_accuracy: 0.7900 - trn_score: 0.6918 - val_score: 0.6938\n",
            "Epoch 21/100\n",
            "2799/2800 [============================>.] - ETA: 0s - loss: 0.4550 - accuracy: 0.8001trn_score 0.7229538601370215 val_score 0.7187982114353524\n",
            "2800/2800 [==============================] - 15s 5ms/step - loss: 0.4549 - accuracy: 0.8002 - val_loss: 0.4772 - val_accuracy: 0.7929 - trn_score: 0.7230 - val_score: 0.7188\n",
            "Epoch 22/100\n",
            "2791/2800 [============================>.] - ETA: 0s - loss: 0.4576 - accuracy: 0.7963trn_score 0.7242867803051981 val_score 0.7007163003710944\n",
            "2800/2800 [==============================] - 15s 5ms/step - loss: 0.4577 - accuracy: 0.7959 - val_loss: 0.4884 - val_accuracy: 0.7857 - trn_score: 0.7243 - val_score: 0.7007\n",
            "Epoch 23/100\n",
            "2791/2800 [============================>.] - ETA: 0s - loss: 0.4498 - accuracy: 0.7990trn_score 0.7039728251983123 val_score 0.7026131679970455\n",
            "Restoring model weights from the end of the best epoch: 13.\n",
            "2800/2800 [==============================] - 15s 5ms/step - loss: 0.4498 - accuracy: 0.7989 - val_loss: 0.4908 - val_accuracy: 0.7893 - trn_score: 0.7040 - val_score: 0.7026\n",
            "Epoch 00023: early stopping\n",
            "\n",
            "F1_0_val is:0.7278481012658228, F1_1_val is:0.8613224107665302, F1_2_val is:0.5882352941176471, total score is : 0.7258019353833333\n",
            "\n",
            "\n",
            "F1_0_val is:0.7505279034690799, F1_1_val is:0.8609365737996444, F1_2_val is:0.6008888888888889, total score is : 0.7374511220525378\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7374511220525378"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQC0p4DUWffy",
        "outputId": "810dc5d7-79d6-42ec-9d76-3723c627add6"
      },
      "source": [
        "%cd COLAB"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/gdrive/My Drive/COLAB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlT27-MMWwZ6",
        "outputId": "4e25f864-6149-4e4b-a157-b16c197fb961"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hy_round1_train_20200102  npy\t    test.feather       train.feather\n",
            "new.ipynb\t\t  RF.ipynb  Tianchi_CNN.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbWRgDioWyyi"
      },
      "source": [
        "test = pd.read_feather('test.feather')"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "HkREUb8fW6pZ",
        "outputId": "65337336-6fec-4efc-f98a-394aed4c0930"
      },
      "source": [
        "test"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>渔船ID</th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "      <th>速度</th>\n",
              "      <th>方向</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7000</td>\n",
              "      <td>7.118845e+06</td>\n",
              "      <td>5.918277e+06</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0</td>\n",
              "      <td>1103 11:54:32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7000</td>\n",
              "      <td>7.118940e+06</td>\n",
              "      <td>5.918285e+06</td>\n",
              "      <td>0.32</td>\n",
              "      <td>346</td>\n",
              "      <td>1103 11:44:32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7000</td>\n",
              "      <td>7.118948e+06</td>\n",
              "      <td>5.918174e+06</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0</td>\n",
              "      <td>1103 11:34:43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7000</td>\n",
              "      <td>7.118948e+06</td>\n",
              "      <td>5.918174e+06</td>\n",
              "      <td>0.11</td>\n",
              "      <td>71</td>\n",
              "      <td>1103 11:14:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7000</td>\n",
              "      <td>7.118948e+06</td>\n",
              "      <td>5.918174e+06</td>\n",
              "      <td>0.11</td>\n",
              "      <td>30</td>\n",
              "      <td>1103 11:04:46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>782373</th>\n",
              "      <td>8999</td>\n",
              "      <td>6.283750e+06</td>\n",
              "      <td>5.284013e+06</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0</td>\n",
              "      <td>1028 00:59:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>782374</th>\n",
              "      <td>8999</td>\n",
              "      <td>6.283750e+06</td>\n",
              "      <td>5.284013e+06</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0</td>\n",
              "      <td>1028 00:49:26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>782375</th>\n",
              "      <td>8999</td>\n",
              "      <td>6.283750e+06</td>\n",
              "      <td>5.284013e+06</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0</td>\n",
              "      <td>1028 00:29:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>782376</th>\n",
              "      <td>8999</td>\n",
              "      <td>6.283750e+06</td>\n",
              "      <td>5.284013e+06</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0</td>\n",
              "      <td>1028 00:19:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>782377</th>\n",
              "      <td>8999</td>\n",
              "      <td>6.283750e+06</td>\n",
              "      <td>5.284013e+06</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0</td>\n",
              "      <td>1028 00:09:04</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>782378 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        渔船ID             x             y    速度   方向           time\n",
              "0       7000  7.118845e+06  5.918277e+06  0.11    0  1103 11:54:32\n",
              "1       7000  7.118940e+06  5.918285e+06  0.32  346  1103 11:44:32\n",
              "2       7000  7.118948e+06  5.918174e+06  0.11    0  1103 11:34:43\n",
              "3       7000  7.118948e+06  5.918174e+06  0.11   71  1103 11:14:30\n",
              "4       7000  7.118948e+06  5.918174e+06  0.11   30  1103 11:04:46\n",
              "...      ...           ...           ...   ...  ...            ...\n",
              "782373  8999  6.283750e+06  5.284013e+06  0.00    0  1028 00:59:02\n",
              "782374  8999  6.283750e+06  5.284013e+06  0.00    0  1028 00:49:26\n",
              "782375  8999  6.283750e+06  5.284013e+06  0.05    0  1028 00:29:04\n",
              "782376  8999  6.283750e+06  5.284013e+06  0.05    0  1028 00:19:04\n",
              "782377  8999  6.283750e+06  5.284013e+06  0.00    0  1028 00:09:04\n",
              "\n",
              "[782378 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ccmwtyTCXZn5",
        "outputId": "f3e3fb67-94fa-49b4-93ca-a8cb86b4064e"
      },
      "source": [
        "test.columns = ['id', 'x', 'y', 'speed', 'ori', 'time']\n",
        "test.head()"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "      <th>speed</th>\n",
              "      <th>ori</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7000</td>\n",
              "      <td>7.118845e+06</td>\n",
              "      <td>5.918277e+06</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0</td>\n",
              "      <td>1103 11:54:32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7000</td>\n",
              "      <td>7.118940e+06</td>\n",
              "      <td>5.918285e+06</td>\n",
              "      <td>0.32</td>\n",
              "      <td>346</td>\n",
              "      <td>1103 11:44:32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7000</td>\n",
              "      <td>7.118948e+06</td>\n",
              "      <td>5.918174e+06</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0</td>\n",
              "      <td>1103 11:34:43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7000</td>\n",
              "      <td>7.118948e+06</td>\n",
              "      <td>5.918174e+06</td>\n",
              "      <td>0.11</td>\n",
              "      <td>71</td>\n",
              "      <td>1103 11:14:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7000</td>\n",
              "      <td>7.118948e+06</td>\n",
              "      <td>5.918174e+06</td>\n",
              "      <td>0.11</td>\n",
              "      <td>30</td>\n",
              "      <td>1103 11:04:46</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     id             x             y  speed  ori           time\n",
              "0  7000  7.118845e+06  5.918277e+06   0.11    0  1103 11:54:32\n",
              "1  7000  7.118940e+06  5.918285e+06   0.32  346  1103 11:44:32\n",
              "2  7000  7.118948e+06  5.918174e+06   0.11    0  1103 11:34:43\n",
              "3  7000  7.118948e+06  5.918174e+06   0.11   71  1103 11:14:30\n",
              "4  7000  7.118948e+06  5.918174e+06   0.11   30  1103 11:04:46"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsAfx3jzXifS"
      },
      "source": [
        "test = feature_engineer(test, test=True)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "BH25TJZvXkyh",
        "outputId": "5bff0cff-3e8f-4b92-dba3-a410dd6a3a16"
      },
      "source": [
        "test[important] = scaler.fit_transform(test[important])\n",
        "test[important]"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>x_std</th>\n",
              "      <th>x_min</th>\n",
              "      <th>x_max</th>\n",
              "      <th>x_mean</th>\n",
              "      <th>y_std</th>\n",
              "      <th>y_min</th>\n",
              "      <th>y_max</th>\n",
              "      <th>y_mean</th>\n",
              "      <th>speed_sin_std</th>\n",
              "      <th>speed_sin_max</th>\n",
              "      <th>speed_sin_mean</th>\n",
              "      <th>speed_cos_std</th>\n",
              "      <th>speed_cos_max</th>\n",
              "      <th>speed_cos_mean</th>\n",
              "      <th>speed_std</th>\n",
              "      <th>speed_max</th>\n",
              "      <th>speed_mean</th>\n",
              "      <th>ori_std</th>\n",
              "      <th>ori_max</th>\n",
              "      <th>ori_mean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.262833</td>\n",
              "      <td>3.069990</td>\n",
              "      <td>3.035617</td>\n",
              "      <td>3.101512</td>\n",
              "      <td>-0.406656</td>\n",
              "      <td>2.393115</td>\n",
              "      <td>2.453454</td>\n",
              "      <td>2.432599</td>\n",
              "      <td>0.363191</td>\n",
              "      <td>0.469540</td>\n",
              "      <td>0.520874</td>\n",
              "      <td>0.538807</td>\n",
              "      <td>0.455983</td>\n",
              "      <td>-0.638705</td>\n",
              "      <td>1.042360</td>\n",
              "      <td>0.021187</td>\n",
              "      <td>-0.144552</td>\n",
              "      <td>0.507427</td>\n",
              "      <td>0.254471</td>\n",
              "      <td>0.379963</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.404578</td>\n",
              "      <td>-0.098783</td>\n",
              "      <td>-0.211853</td>\n",
              "      <td>-0.138093</td>\n",
              "      <td>0.066743</td>\n",
              "      <td>-0.237080</td>\n",
              "      <td>-0.217537</td>\n",
              "      <td>-0.234868</td>\n",
              "      <td>0.466428</td>\n",
              "      <td>0.518821</td>\n",
              "      <td>-0.292704</td>\n",
              "      <td>1.105678</td>\n",
              "      <td>0.822715</td>\n",
              "      <td>0.113178</td>\n",
              "      <td>0.467624</td>\n",
              "      <td>0.021187</td>\n",
              "      <td>0.860456</td>\n",
              "      <td>0.077351</td>\n",
              "      <td>0.180373</td>\n",
              "      <td>0.624419</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.944927</td>\n",
              "      <td>1.199837</td>\n",
              "      <td>1.632259</td>\n",
              "      <td>1.530079</td>\n",
              "      <td>1.641883</td>\n",
              "      <td>0.528292</td>\n",
              "      <td>0.983135</td>\n",
              "      <td>0.785946</td>\n",
              "      <td>0.645973</td>\n",
              "      <td>0.212303</td>\n",
              "      <td>1.821421</td>\n",
              "      <td>0.553906</td>\n",
              "      <td>0.507174</td>\n",
              "      <td>2.347646</td>\n",
              "      <td>0.719707</td>\n",
              "      <td>0.021187</td>\n",
              "      <td>0.797401</td>\n",
              "      <td>0.425439</td>\n",
              "      <td>0.235947</td>\n",
              "      <td>0.820567</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.129142</td>\n",
              "      <td>-0.343044</td>\n",
              "      <td>-0.397233</td>\n",
              "      <td>-0.436262</td>\n",
              "      <td>-0.430396</td>\n",
              "      <td>-0.244431</td>\n",
              "      <td>-0.349826</td>\n",
              "      <td>-0.272895</td>\n",
              "      <td>0.446298</td>\n",
              "      <td>0.575161</td>\n",
              "      <td>-0.041552</td>\n",
              "      <td>-0.108925</td>\n",
              "      <td>0.110184</td>\n",
              "      <td>0.000014</td>\n",
              "      <td>0.836987</td>\n",
              "      <td>0.021187</td>\n",
              "      <td>-0.515798</td>\n",
              "      <td>0.661055</td>\n",
              "      <td>0.254471</td>\n",
              "      <td>0.078360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.072288</td>\n",
              "      <td>0.393188</td>\n",
              "      <td>0.360064</td>\n",
              "      <td>0.311796</td>\n",
              "      <td>0.001638</td>\n",
              "      <td>0.376797</td>\n",
              "      <td>0.388380</td>\n",
              "      <td>0.466091</td>\n",
              "      <td>0.296491</td>\n",
              "      <td>0.661817</td>\n",
              "      <td>0.571083</td>\n",
              "      <td>0.306650</td>\n",
              "      <td>0.284893</td>\n",
              "      <td>-0.275893</td>\n",
              "      <td>0.868187</td>\n",
              "      <td>0.021187</td>\n",
              "      <td>-0.274008</td>\n",
              "      <td>0.544657</td>\n",
              "      <td>0.161849</td>\n",
              "      <td>0.110225</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>1.272909</td>\n",
              "      <td>0.448762</td>\n",
              "      <td>0.813073</td>\n",
              "      <td>0.434242</td>\n",
              "      <td>-0.564160</td>\n",
              "      <td>0.631898</td>\n",
              "      <td>0.476017</td>\n",
              "      <td>0.536370</td>\n",
              "      <td>1.310504</td>\n",
              "      <td>-0.028098</td>\n",
              "      <td>-2.264725</td>\n",
              "      <td>-0.229295</td>\n",
              "      <td>0.822326</td>\n",
              "      <td>-0.305646</td>\n",
              "      <td>1.530288</td>\n",
              "      <td>0.021187</td>\n",
              "      <td>0.095520</td>\n",
              "      <td>0.388622</td>\n",
              "      <td>0.180373</td>\n",
              "      <td>0.821723</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>-0.396299</td>\n",
              "      <td>0.525873</td>\n",
              "      <td>0.411600</td>\n",
              "      <td>0.493597</td>\n",
              "      <td>0.624527</td>\n",
              "      <td>0.386501</td>\n",
              "      <td>0.529850</td>\n",
              "      <td>0.411957</td>\n",
              "      <td>0.184931</td>\n",
              "      <td>0.668171</td>\n",
              "      <td>-0.182833</td>\n",
              "      <td>1.381210</td>\n",
              "      <td>0.783881</td>\n",
              "      <td>0.274104</td>\n",
              "      <td>0.353567</td>\n",
              "      <td>0.021187</td>\n",
              "      <td>0.969127</td>\n",
              "      <td>0.187911</td>\n",
              "      <td>0.235947</td>\n",
              "      <td>0.889243</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>-0.736703</td>\n",
              "      <td>0.556438</td>\n",
              "      <td>0.306443</td>\n",
              "      <td>0.446030</td>\n",
              "      <td>-0.770590</td>\n",
              "      <td>0.858814</td>\n",
              "      <td>0.642283</td>\n",
              "      <td>0.758270</td>\n",
              "      <td>-1.602782</td>\n",
              "      <td>-1.452558</td>\n",
              "      <td>0.012218</td>\n",
              "      <td>-1.608907</td>\n",
              "      <td>-1.637190</td>\n",
              "      <td>0.040152</td>\n",
              "      <td>-1.760166</td>\n",
              "      <td>-1.513177</td>\n",
              "      <td>-1.272015</td>\n",
              "      <td>-2.916314</td>\n",
              "      <td>-0.060445</td>\n",
              "      <td>-2.286806</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>-0.145311</td>\n",
              "      <td>-0.130097</td>\n",
              "      <td>-0.178028</td>\n",
              "      <td>-0.150768</td>\n",
              "      <td>1.458013</td>\n",
              "      <td>-0.457088</td>\n",
              "      <td>-0.218293</td>\n",
              "      <td>-0.273590</td>\n",
              "      <td>-0.063972</td>\n",
              "      <td>0.281910</td>\n",
              "      <td>-0.285366</td>\n",
              "      <td>0.560433</td>\n",
              "      <td>-0.433942</td>\n",
              "      <td>-1.623616</td>\n",
              "      <td>0.572493</td>\n",
              "      <td>0.021187</td>\n",
              "      <td>0.098524</td>\n",
              "      <td>0.254146</td>\n",
              "      <td>0.217422</td>\n",
              "      <td>0.245763</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>-0.735580</td>\n",
              "      <td>0.150931</td>\n",
              "      <td>-0.103776</td>\n",
              "      <td>0.031596</td>\n",
              "      <td>-0.768013</td>\n",
              "      <td>0.156667</td>\n",
              "      <td>-0.062688</td>\n",
              "      <td>0.035580</td>\n",
              "      <td>-1.584258</td>\n",
              "      <td>-1.427678</td>\n",
              "      <td>0.006610</td>\n",
              "      <td>-1.601653</td>\n",
              "      <td>-1.583989</td>\n",
              "      <td>-0.005587</td>\n",
              "      <td>-1.742756</td>\n",
              "      <td>-1.479993</td>\n",
              "      <td>-1.287878</td>\n",
              "      <td>-2.835202</td>\n",
              "      <td>-0.190117</td>\n",
              "      <td>-2.284700</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows × 20 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         x_std     x_min     x_max  ...   ori_std   ori_max  ori_mean\n",
              "0    -0.262833  3.069990  3.035617  ...  0.507427  0.254471  0.379963\n",
              "1    -0.404578 -0.098783 -0.211853  ...  0.077351  0.180373  0.624419\n",
              "2     0.944927  1.199837  1.632259  ...  0.425439  0.235947  0.820567\n",
              "3    -0.129142 -0.343044 -0.397233  ...  0.661055  0.254471  0.078360\n",
              "4    -0.072288  0.393188  0.360064  ...  0.544657  0.161849  0.110225\n",
              "...        ...       ...       ...  ...       ...       ...       ...\n",
              "1995  1.272909  0.448762  0.813073  ...  0.388622  0.180373  0.821723\n",
              "1996 -0.396299  0.525873  0.411600  ...  0.187911  0.235947  0.889243\n",
              "1997 -0.736703  0.556438  0.306443  ... -2.916314 -0.060445 -2.286806\n",
              "1998 -0.145311 -0.130097 -0.178028  ...  0.254146  0.217422  0.245763\n",
              "1999 -0.735580  0.150931 -0.103776  ... -2.835202 -0.190117 -2.284700\n",
              "\n",
              "[2000 rows x 20 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZrcCZuwXn5R"
      },
      "source": [
        "y_preds = []\n",
        "for model in models:\n",
        "    y_preds.append(model.predict(train[important]))\n",
        "y = np.mean(y_preds, axis=0)"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FFztxm3XzeR",
        "outputId": "e00a77a0-6840-4ce3-87a3-f5941c17b8ef"
      },
      "source": [
        "y_ = np.zeros([len(y), 1])\n",
        "\n",
        "for i in range(len(y)):\n",
        "    y_[i] = list(y[i]).index(max(y[i]))\n",
        "y_, len(y)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        ...,\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.]]), 7000)"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        },
        "id": "B2aC55KFYtKZ",
        "outputId": "5a4abf61-53cb-424e-d51e-d01f70ca3fc2"
      },
      "source": [
        "cm_plot(original_label=train['type'], predict_label=y_, pic=None)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 288x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jM_V6Ydx1NpX",
        "outputId": "ebeaca1f-4666-46be-bbd0-54bef670f4fe"
      },
      "source": [
        "clear(str)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[H\u001b[2J"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXK3oKE8yW9H"
      },
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "#plot_model(models[0], to_file=\"lenet.png\", show_shapes=True)\n",
        "i = 1\n",
        "for model in models:\n",
        "  s = str(i)+'.png'\n",
        "  i += 1\n",
        "  plot_model(model,to_file=s,show_shapes=True)"
      ],
      "execution_count": 90,
      "outputs": []
    }
  ]
}